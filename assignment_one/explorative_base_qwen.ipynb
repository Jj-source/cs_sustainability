{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "72gErY_lDa7h",
    "outputId": "80e093f6-5371-4d11-dbe3-234b3077159c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: qwen-vl-utils in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (0.0.14)\n",
      "Requirement already satisfied: datasets in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (4.2.0)\n",
      "Requirement already satisfied: torch in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (2.9.0)\n",
      "Requirement already satisfied: datasets in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (4.2.0)\n",
      "Requirement already satisfied: torch in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (2.9.0)\n",
      "Requirement already satisfied: torchvision in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (0.24.0)\n",
      "Requirement already satisfied: torchvision in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (0.24.0)\n",
      "Requirement already satisfied: transformers in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (4.57.1)\n",
      "Requirement already satisfied: accelerate in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (1.11.0)\n",
      "Requirement already satisfied: transformers in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (4.57.1)\n",
      "Requirement already satisfied: accelerate in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (1.11.0)\n",
      "Requirement already satisfied: av in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from qwen-vl-utils) (16.0.1)\n",
      "Requirement already satisfied: packaging in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from qwen-vl-utils) (25.0)\n",
      "Requirement already satisfied: pillow in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from qwen-vl-utils) (12.0.0)\n",
      "Requirement already satisfied: requests in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from qwen-vl-utils) (2.32.5)\n",
      "Requirement already satisfied: filelock in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from datasets) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from datasets) (2.3.4)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2025.9.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from datasets) (0.35.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.13.1)\n",
      "Requirement already satisfied: anyio in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: certifi in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.1.10)\n",
      "Requirement already satisfied: setuptools in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.0 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from torch) (3.5.0)\n",
      "Requirement already satisfied: av in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from qwen-vl-utils) (16.0.1)\n",
      "Requirement already satisfied: packaging in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from qwen-vl-utils) (25.0)\n",
      "Requirement already satisfied: pillow in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from qwen-vl-utils) (12.0.0)\n",
      "Requirement already satisfied: requests in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from qwen-vl-utils) (2.32.5)\n",
      "Requirement already satisfied: filelock in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from datasets) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from datasets) (2.3.4)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2025.9.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from datasets) (0.35.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.13.1)\n",
      "Requirement already satisfied: anyio in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: certifi in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.1.10)\n",
      "Requirement already satisfied: setuptools in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.0 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from torch) (3.5.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from transformers) (2025.10.22)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: psutil in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from accelerate) (7.1.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from requests->qwen-vl-utils) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from requests->qwen-vl-utils) (2.5.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from transformers) (2025.10.22)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: psutil in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from accelerate) (7.1.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from requests->qwen-vl-utils) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from requests->qwen-vl-utils) (2.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: ipywidgets in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (8.1.7)\n",
      "Requirement already satisfied: ipywidgets in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (8.1.7)\n",
      "Requirement already satisfied: widgetsnbextension in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (4.0.14)\n",
      "Requirement already satisfied: widgetsnbextension in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (4.0.14)\n",
      "Requirement already satisfied: comm>=0.1.3 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from ipywidgets) (9.6.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from ipywidgets) (3.0.15)\n",
      "Requirement already satisfied: decorator in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
      "Requirement already satisfied: stack_data in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: wcwidth in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.14)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.5)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: comm>=0.1.3 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from ipywidgets) (9.6.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from ipywidgets) (3.0.15)\n",
      "Requirement already satisfied: decorator in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
      "Requirement already satisfied: stack_data in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: wcwidth in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.14)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.5)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /home/jj/github/appunti/.venv/lib/python3.12/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "usage: jupyter [-h] [--version] [--config-dir] [--data-dir] [--runtime-dir]\n",
      "               [--paths] [--json] [--debug]\n",
      "               [subcommand]\n",
      "\n",
      "Jupyter: Interactive Computing\n",
      "\n",
      "positional arguments:\n",
      "  subcommand     the subcommand to launch\n",
      "\n",
      "options:\n",
      "  -h, --help     show this help message and exit\n",
      "  --version      show the versions of core jupyter packages and exit\n",
      "  --config-dir   show Jupyter config dir\n",
      "  --data-dir     show Jupyter data dir\n",
      "  --runtime-dir  show Jupyter runtime dir\n",
      "  --paths        show all Jupyter paths. Add --json for machine-readable\n",
      "                 format.\n",
      "  --json         output paths as machine-readable json\n",
      "  --debug        output debug information about paths\n",
      "\n",
      "Available subcommands: console dejavu events execute kernel kernelspec lab\n",
      "labextension labhub migrate nbconvert notebook run server troubleshoot trust\n",
      "\n",
      "Jupyter command `jupyter-nbextension` not found.\n",
      "usage: jupyter [-h] [--version] [--config-dir] [--data-dir] [--runtime-dir]\n",
      "               [--paths] [--json] [--debug]\n",
      "               [subcommand]\n",
      "\n",
      "Jupyter: Interactive Computing\n",
      "\n",
      "positional arguments:\n",
      "  subcommand     the subcommand to launch\n",
      "\n",
      "options:\n",
      "  -h, --help     show this help message and exit\n",
      "  --version      show the versions of core jupyter packages and exit\n",
      "  --config-dir   show Jupyter config dir\n",
      "  --data-dir     show Jupyter data dir\n",
      "  --runtime-dir  show Jupyter runtime dir\n",
      "  --paths        show all Jupyter paths. Add --json for machine-readable\n",
      "                 format.\n",
      "  --json         output paths as machine-readable json\n",
      "  --debug        output debug information about paths\n",
      "\n",
      "Available subcommands: console dejavu events execute kernel kernelspec lab\n",
      "labextension labhub migrate nbconvert notebook run server troubleshoot trust\n",
      "\n",
      "Jupyter command `jupyter-nbextension` not found.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages (works for both Colab and local Jupyter)\n",
    "!pip install -U qwen-vl-utils datasets torch torchvision transformers accelerate\n",
    "!pip install -U ipywidgets widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "r_vC1rxOCiVZ"
   },
   "outputs": [],
   "source": [
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# evaluation\n",
    "import os\n",
    "import re\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "1fYYLSAjZ85L"
   },
   "outputs": [],
   "source": [
    "## setting of important macros\n",
    "MODEL_ID = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
    "SUBSET_SIZE = 50\n",
    "SHUFFLE_BUFFER_SIZE = SUBSET_SIZE * 10\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122,
     "referenced_widgets": [
      "076c0d24260f4f18a43a9558ce664cb2",
      "2c22bae211bb4b3ab440c1ee987dd544",
      "aee2363983ee4647a2aff35edb6f35d1",
      "67a58360f1fe4789be2e9ab67695a3dc",
      "f32519912fa34ff99ff31367cae10338",
      "d84382de4ad64141b92859057bda4b4a",
      "7ffe2e595eea48ac81995d3c71c1af38",
      "668bcd14eacf47638aecee5a3fe71330",
      "0826cf8dd39b493c9679db0cab2a9674",
      "d37b98ed92e1493091ba99c3acea4660",
      "8910c01866dd4650a5bab066f517d96e"
     ]
    },
    "id": "QnQiZ8tpVWPw",
    "outputId": "8d59b49c-f381-4187-def3-2fcc85815460"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading VQAv2 dataset with streaming...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eebc70cfa47d42568523c62b3b7bd691",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffle the dataset, buffer size of 500...\n",
      "Taking subset of 50 samples...\n",
      "Loaded 50 samples\n",
      "Loaded 50 samples\n"
     ]
    }
   ],
   "source": [
    "# Load VQAv2 dataset - using the mirror version that doesn't require dataset scripts\n",
    "print(\"Loading VQAv2 dataset with streaming...\")\n",
    "dataset = load_dataset(\"Multimodal-Fatima/VQAv2_validation\", split=\"validation\", streaming=True)\n",
    "\n",
    "# Shuffle dataset before taking subset to ensure representative sampling\n",
    "print(f\"Shuffle the dataset, buffer size of {SHUFFLE_BUFFER_SIZE}...\")\n",
    "shuffled_dataset = dataset.shuffle(seed=SEED, buffer_size=SHUFFLE_BUFFER_SIZE)\n",
    "\n",
    "# Take subset and convert to list (needed for multiple iterations)\n",
    "print(f\"Taking subset of {SUBSET_SIZE} samples...\")\n",
    "samples = list(shuffled_dataset.take(SUBSET_SIZE))\n",
    "print(f\"Loaded {len(samples)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nw85-jT0XyLC",
    "outputId": "0913e13e-c906-44f0-f68d-02d1d3ed25a9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question_type': 'how many',\n",
       " 'multiple_choice_answer': '1',\n",
       " 'answers': ['1', '1', '1', '1', '1', '1', '1', '1', '1', '1'],\n",
       " 'answers_original': [{'answer': '1',\n",
       "   'answer_confidence': 'yes',\n",
       "   'answer_id': 1},\n",
       "  {'answer': '1', 'answer_confidence': 'yes', 'answer_id': 2},\n",
       "  {'answer': '1', 'answer_confidence': 'yes', 'answer_id': 3},\n",
       "  {'answer': '1', 'answer_confidence': 'yes', 'answer_id': 4},\n",
       "  {'answer': '1', 'answer_confidence': 'yes', 'answer_id': 5},\n",
       "  {'answer': '1', 'answer_confidence': 'yes', 'answer_id': 6},\n",
       "  {'answer': '1', 'answer_confidence': 'yes', 'answer_id': 7},\n",
       "  {'answer': '1', 'answer_confidence': 'yes', 'answer_id': 8},\n",
       "  {'answer': '1', 'answer_confidence': 'yes', 'answer_id': 9},\n",
       "  {'answer': '1', 'answer_confidence': 'yes', 'answer_id': 10}],\n",
       " 'id_image': 232309,\n",
       " 'answer_type': 'number',\n",
       " 'question_id': 232309002,\n",
       " 'question': 'How many kites are in the picture?',\n",
       " 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=500x332>,\n",
       " 'id': 169159,\n",
       " 'clip_tags_ViT_L_14': ['stunt kite',\n",
       "  'large kite flying',\n",
       "  'kiteportion',\n",
       "  '\"large kite',\n",
       "  'kite boarding'],\n",
       " 'blip_caption': 'a man flying a kite on the beach',\n",
       " 'LLM_Description_gpt3_downstream_tasks_visual_genome_ViT_L_14': ['kite-pulling',\n",
       "  'maneuvering the kite',\n",
       "  'string or cord connecting the kite to a person',\n",
       "  'kite-boarding',\n",
       "  'kite boarding'],\n",
       " 'DETA_detections_deta_swin_large_o365_coco_classes': [{'attribute': '',\n",
       "   'box': [120.47334289550781,\n",
       "    121.14210510253906,\n",
       "    226.51441955566406,\n",
       "    293.7930603027344],\n",
       "   'label': 'Person',\n",
       "   'location': 'middle',\n",
       "   'ratio': 0.11028972268104553,\n",
       "   'size': 'small',\n",
       "   'tag': ''},\n",
       "  {'attribute': '',\n",
       "   'box': [121.32807922363281,\n",
       "    270.19610595703125,\n",
       "    139.09259033203125,\n",
       "    293.84442138671875],\n",
       "   'label': 'Sneakers',\n",
       "   'location': 'lower left',\n",
       "   'ratio': 0.0025307275354862213,\n",
       "   'size': 'small',\n",
       "   'tag': ''},\n",
       "  {'attribute': '',\n",
       "   'box': [177.25653076171875,\n",
       "    284.4181213378906,\n",
       "    206.15231323242188,\n",
       "    290.9072570800781],\n",
       "   'label': 'Sneakers',\n",
       "   'location': 'lower middle',\n",
       "   'ratio': 0.001129570184275508,\n",
       "   'size': 'small',\n",
       "   'tag': ''},\n",
       "  {'attribute': '',\n",
       "   'box': [4.800379276275635,\n",
       "    37.91737747192383,\n",
       "    377.7641906738281,\n",
       "    296.6324157714844],\n",
       "   'label': 'Kite',\n",
       "   'location': 'middle',\n",
       "   'ratio': 0.5812731385231018,\n",
       "   'size': 'large',\n",
       "   'tag': ''},\n",
       "  {'attribute': '',\n",
       "   'box': [222.48748779296875,\n",
       "    36.33881759643555,\n",
       "    353.46234130859375,\n",
       "    205.1005401611328],\n",
       "   'label': 'Kite',\n",
       "   'location': 'middle',\n",
       "   'ratio': 0.13315385580062866,\n",
       "   'size': 'small',\n",
       "   'tag': ''}],\n",
       " 'DETA_detections_deta_swin_large_o365_clip_ViT_L_14': [{'attribute': 'string or cord connecting the kite to a person',\n",
       "   'box': [120.47334289550781,\n",
       "    121.14210510253906,\n",
       "    226.51441955566406,\n",
       "    293.7930603027344],\n",
       "   'label': 'Person',\n",
       "   'location': 'middle',\n",
       "   'ratio': 0.11028972268104553,\n",
       "   'size': 'small',\n",
       "   'tag': 'person flying kite'},\n",
       "  {'attribute': 'black markings on the feet and legs',\n",
       "   'box': [121.32807922363281,\n",
       "    270.19610595703125,\n",
       "    139.09259033203125,\n",
       "    293.84442138671875],\n",
       "   'label': 'Sneakers',\n",
       "   'location': 'lower left',\n",
       "   'ratio': 0.0025307275354862213,\n",
       "   'size': 'small',\n",
       "   'tag': 'black/brown shoe'},\n",
       "  {'attribute': 'kite-flying',\n",
       "   'box': [4.800379276275635,\n",
       "    37.91737747192383,\n",
       "    377.7641906738281,\n",
       "    296.6324157714844],\n",
       "   'label': 'Kite',\n",
       "   'location': 'middle',\n",
       "   'ratio': 0.5812731385231018,\n",
       "   'size': 'large',\n",
       "   'tag': 'large kite flying'},\n",
       "  {'attribute': 'dragon kite',\n",
       "   'box': [222.48748779296875,\n",
       "    36.33881759643555,\n",
       "    353.46234130859375,\n",
       "    205.1005401611328],\n",
       "   'label': 'Kite',\n",
       "   'location': 'middle',\n",
       "   'ratio': 0.13315385580062866,\n",
       "   'size': 'small',\n",
       "   'tag': 'dragon kite'}],\n",
       " 'DETA_detections_deta_swin_large_o365_clip_ViT_L_14_blip_caption': [{'attribute': 'string or cord connecting the kite to a person',\n",
       "   'box': [120.47334289550781,\n",
       "    121.14210510253906,\n",
       "    226.51441955566406,\n",
       "    293.7930603027344],\n",
       "   'caption': 'a man on the beach with a kite',\n",
       "   'label': 'Person',\n",
       "   'location': 'middle',\n",
       "   'ratio': 0.11028972268104553,\n",
       "   'size': 'small',\n",
       "   'tag': 'person flying kite'},\n",
       "  {'attribute': 'black markings on the feet and legs',\n",
       "   'box': [121.32807922363281,\n",
       "    270.19610595703125,\n",
       "    139.09259033203125,\n",
       "    293.84442138671875],\n",
       "   'caption': 'a black cat on the floor',\n",
       "   'label': 'Sneakers',\n",
       "   'location': 'lower left',\n",
       "   'ratio': 0.0025307275354862213,\n",
       "   'size': 'small',\n",
       "   'tag': 'black/brown shoe'},\n",
       "  {'attribute': 'kite-flying',\n",
       "   'box': [4.800379276275635,\n",
       "    37.91737747192383,\n",
       "    377.7641906738281,\n",
       "    296.6324157714844],\n",
       "   'caption': 'a man flying a kite on the beach',\n",
       "   'label': 'Kite',\n",
       "   'location': 'middle',\n",
       "   'ratio': 0.5812731385231018,\n",
       "   'size': 'large',\n",
       "   'tag': 'large kite flying'},\n",
       "  {'attribute': 'dragon kite',\n",
       "   'box': [222.48748779296875,\n",
       "    36.33881759643555,\n",
       "    353.46234130859375,\n",
       "    205.1005401611328],\n",
       "   'caption': 'a person flying a kite on the beach',\n",
       "   'label': 'Kite',\n",
       "   'location': 'middle',\n",
       "   'ratio': 0.13315385580062866,\n",
       "   'size': 'small',\n",
       "   'tag': 'dragon kite'}],\n",
       " 'Attributes_ViT_L_14_descriptors_text_davinci_003_full': ['kite which has brightly colored fabric or paper stretched over the frame',\n",
       "  'kite which is a light frame made of wood, plastic, or fiberglass',\n",
       "  'kite which is a tail made of fabric, string, or ribbon',\n",
       "  'kite (bird of prey) which is a medium-sized, graceful bird of prey',\n",
       "  'kite (bird of prey) which has hooked beak'],\n",
       " 'clip_tags_ViT_L_14_wo_openai': ['kite',\n",
       "  'kite',\n",
       "  'kite',\n",
       "  'parasail',\n",
       "  'glider'],\n",
       " 'clip_tags_ViT_L_14_with_openai': ['Kite sports',\n",
       "  'Sport kite',\n",
       "  'Kite landboarding',\n",
       "  'Kiteboating',\n",
       "  'Kite buggy'],\n",
       " 'clip_tags_LAION_ViT_H_14_2B_wo_openai': ['kite',\n",
       "  'kite',\n",
       "  'kite',\n",
       "  'windsock',\n",
       "  'windsock'],\n",
       " 'clip_tags_LAION_ViT_H_14_2B_with_openai': ['Kite sports',\n",
       "  'Kite skating',\n",
       "  'kite',\n",
       "  'Kite',\n",
       "  'kite'],\n",
       " 'clip_tags_LAION_ViT_bigG_14_2B_wo_openai': ['kite',\n",
       "  'kite',\n",
       "  'kite',\n",
       "  'windsock',\n",
       "  'windsock'],\n",
       " 'clip_tags_LAION_ViT_bigG_14_2B_with_openai': ['Kite skating',\n",
       "  'Sport kite',\n",
       "  'kite',\n",
       "  'kite',\n",
       "  'Kite'],\n",
       " 'Attributes_LAION_ViT_H_14_2B_descriptors_text_davinci_003_full': ['kite which is a light frame made of wood, plastic, or fiberglass',\n",
       "  'kite which is a tail made of fabric, string, or ribbon',\n",
       "  'kite which is a bridle attached to the frame that holds the kite in place',\n",
       "  'kite which has brightly colored fabric or paper stretched over the frame',\n",
       "  'kite which is a line to fly the kite attached to the bridle'],\n",
       " 'Attributes_LAION_ViT_bigG_14_2B_descriptors_text_davinci_003_full': ['kite which is a light frame made of wood, plastic, or fiberglass',\n",
       "  'kite which is a tail made of fabric, string, or ribbon',\n",
       "  'kite which is a line to fly the kite attached to the bridle',\n",
       "  'kite which has brightly colored fabric or paper stretched over the frame',\n",
       "  'kite which is a bridle attached to the frame that holds the kite in place'],\n",
       " 'DETA_detections_deta_swin_large_o365_coco_classes_caption_module_random': [{'attribute': '',\n",
       "   'box': [120.47334289550781,\n",
       "    121.14210510253906,\n",
       "    226.51441955566406,\n",
       "    293.7930603027344],\n",
       "   'captions_module': ['the man stands on the beach as he holds a kite',\n",
       "    'the guy is walking along the beach with his red and white kite',\n",
       "    'a man with a tie playing a game of kites on the beach',\n",
       "    'a person running on a beach with a kite',\n",
       "    'a male is being chased by an octopus shaped kite',\n",
       "    'a man on a beach flying a kite',\n",
       "    'man holding a kite next to a blue white and red kite',\n",
       "    'a man walking on beach walking with kite in hand',\n",
       "    'a man standing on a beach holding a kite',\n",
       "    'a man with a red kite is walking on sand',\n",
       "    'a man walking with an orange and black and red kite',\n",
       "    'a man flying a kite along a beach on a cloudy day',\n",
       "    'a man is walking near a beach holding a ribbon and kite',\n",
       "    'a man standing on top of a beach next to a kite',\n",
       "    'a man is on the beach with a kite',\n",
       "    'a man at the beach runs toward a red kite in his hand',\n",
       "    'a man is on a beach flying a kite',\n",
       "    'man flying a kite beside water near shore line',\n",
       "    'a man in shorts is flying a kite by the ocean',\n",
       "    'a man flying a kite on the beach',\n",
       "    'a man is walking in the sand with a kite',\n",
       "    'a man walking on a beach with a kite',\n",
       "    'a man standing on a beach flying a kite',\n",
       "    'a man on the beach flying a kite',\n",
       "    'a man running on a beach holding a kite',\n",
       "    'a man wearing a blue shirt flying a kite',\n",
       "    'an older man walking on the beach with a kite',\n",
       "    'a man who is walking with a kite in his hand on a beach',\n",
       "    'a man is standing on a beach flying a kite',\n",
       "    'a man running on the beach after getting his kite out of the water'],\n",
       "   'captions_module_filter': ['a man plays with streamers in a sea of water',\n",
       "    'adult walking by water with young boy on beach',\n",
       "    'this person is flying a kite on the beach',\n",
       "    'there is a man walking on a beach with a kite',\n",
       "    'a man is running along a wave breaking sandy beach',\n",
       "    'a man on a beach flies a kite on a beach',\n",
       "    'a person on the sand with a red kite',\n",
       "    'a man tries to get a kite from him',\n",
       "    'a person is going to fly a kite on the sandy beach',\n",
       "    'a man is on the beach flying a kite',\n",
       "    'a man with wet hair standing in the sand flying a red kite',\n",
       "    'a man walking on top of a beach next to the ocean',\n",
       "    'there is a man on the beach that is about to fly a kite',\n",
       "    'a man on a beach with a kite and waves behind him',\n",
       "    'a man flying a kite at the beach',\n",
       "    'a man walking on a sandy beach with a kite',\n",
       "    'a man flying a kite on a sand beach',\n",
       "    'a man is holding on to a string',\n",
       "    'this is an outdoor picture of a man at the beach',\n",
       "    'there is a male in a blue shirt flying a kite',\n",
       "    'a person walking by the water flying a kite',\n",
       "    'the man wears a blue shirt and the bright red tail band tie',\n",
       "    'a man on the beach with a kite in his hand',\n",
       "    'a man pulls a kite in both directions on the beach',\n",
       "    'a guy running on the beach with a kite',\n",
       "    'a person is walking on a beach while holding two ribbons with one end of the red blue and orange band',\n",
       "    'a man walking down a beach with a kite',\n",
       "    'a man on a beach with kite flying in the air',\n",
       "    'an image of a man standing on the beach flying a kite',\n",
       "    'a man is flying a blue kite on the beach'],\n",
       "   'label': 'Person',\n",
       "   'location': 'middle',\n",
       "   'ratio': 0.11028972268104553,\n",
       "   'size': 'small',\n",
       "   'tag': ''},\n",
       "  {'attribute': '',\n",
       "   'box': [121.32807922363281,\n",
       "    270.19610595703125,\n",
       "    139.09259033203125,\n",
       "    293.84442138671875],\n",
       "   'captions_module': ['a black shoe standing near a small cat',\n",
       "    'a person in jeans laying on a counter',\n",
       "    'a cat is sitting down on a stone shelf next to a persons feet',\n",
       "    'a man in a cap sitting in front of some books under a laptop',\n",
       "    'a dog laying down on a bathroom counter top',\n",
       "    'a man in black shoes is on the toilet',\n",
       "    'a close up of a white toilet with a person sitting next to it',\n",
       "    'a large black dog laying in its bed',\n",
       "    'a black cat sitting on top of foot pads',\n",
       "    'there is a dog laying on a blanket near a persons legs',\n",
       "    \"a person's hand holding a cat in their lap with a cat sitting on its legs\",\n",
       "    \"a close up of a person's shoe on the side of a road\",\n",
       "    'a gray cat stretching its head over a cupcake saucer on a sink',\n",
       "    'the person has their shoes off of the table',\n",
       "    'a black cat sitting on a kitchen counter next to a towel',\n",
       "    'a cat looks down, with a head resting on a sink top',\n",
       "    'a black and white photograph of a person on a computer',\n",
       "    'a black and white photo of a person walking',\n",
       "    'a black and white cat sleeping on a chair in the corner',\n",
       "    'a dog that is laying down on a floor',\n",
       "    'the foot of a person standing on an open gas stove',\n",
       "    'a black dog that is sitting in a chair',\n",
       "    'a black kitten sleeping on a floor with a cup and saucer in front of it',\n",
       "    'a white cat is lying next to a black laptop',\n",
       "    'a cat curled up by a window while sitting',\n",
       "    'a black cat sitting on top of a toilet brush',\n",
       "    'there is a pair of shoes with one one with both shoes open',\n",
       "    'a person using a pair of shoes to take a bath',\n",
       "    'black and white dog looking down at its left',\n",
       "    'a pair of black boots with white socks hanging down'],\n",
       "   'captions_module_filter': ['a cat on a chair in a bathroom',\n",
       "    'two cats resting on a chair on a black and white picture by',\n",
       "    'a black dog on a white and gray microwave oven',\n",
       "    'a black and white cat standing in an old, dirty urinal',\n",
       "    'a black dog, black boots sits in a chair',\n",
       "    'a black dog lays on a white counter',\n",
       "    'a large dog laying on a pair of shoes',\n",
       "    'a person sitting on a shoe with their shoe on',\n",
       "    'a black cat is sniffing a cat in the bathroom',\n",
       "    'there is a dog taking a nap in a bathtub',\n",
       "    'a black and white cat with its paw on a foot',\n",
       "    'a big black cat sitting on the floor in the bathroom',\n",
       "    'a picture of a cat lying on its owners foot',\n",
       "    'black and white cat reaching forward with paw on table',\n",
       "    'a picture of a cat that is sitting under a person',\n",
       "    'a dog is standing on the edge of a bathroom sink',\n",
       "    'a black cat standing at the seat of a black chair',\n",
       "    'a black cat stretching itself its paws out over a toilet',\n",
       "    'a cat sitting on top of a pair of heels',\n",
       "    'the feet of a person who is squatting down',\n",
       "    'a dog on a white towel and a toilet with a sink',\n",
       "    'sandals laying on a bench with the sole up',\n",
       "    \"man's foot and shoes taken from above\",\n",
       "    'a cat sitting on top of a black sneakers with a white kitten in front of a pair of socks',\n",
       "    'a dog peeking his head over a toilet seat',\n",
       "    'a cat is laying on the toilet seat and hiding',\n",
       "    'a cat sitting on top of a floor next to a person',\n",
       "    'a cat is peeking down at a pair of shoes',\n",
       "    'a black and white cats sitting on a tiled floor',\n",
       "    'a black cat standing on top of a kitchen counter'],\n",
       "   'label': 'Sneakers',\n",
       "   'location': 'lower left',\n",
       "   'ratio': 0.0025307275354862213,\n",
       "   'size': 'small',\n",
       "   'tag': ''},\n",
       "  {'attribute': '',\n",
       "   'box': [177.25653076171875,\n",
       "    284.4181213378906,\n",
       "    206.15231323242188,\n",
       "    290.9072570800781],\n",
       "   'captions_module': ['a living room with a chair and television is seen',\n",
       "    'the view of a tall television in a room',\n",
       "    'a table with multiple plates and some bananas',\n",
       "    'a blurry picture of three black chairs in a living room',\n",
       "    'a blurry photo of a living room and tv',\n",
       "    'two cups of smooth blueberries on a table',\n",
       "    'a picture of a kitchen scene with focus on the food',\n",
       "    'a little girl with her eyes closed in a kitchen with an opening curtain',\n",
       "    'a small toy is standing in a room with a window',\n",
       "    'the television is turned on and sitting on a stand',\n",
       "    'a room that has a television and a door',\n",
       "    'a blurry picture of a tv that is on',\n",
       "    'the small blue device is ready to be put on',\n",
       "    'a close up view of a clock on a table',\n",
       "    'the view of a man standing in a room from a blurred backdrop',\n",
       "    'a blurry image of a table with flowers and a large black clock',\n",
       "    'a white dog is running across a tiled hallway',\n",
       "    'a photo of a building from the street',\n",
       "    'a man standing in a hallway holding out his hand',\n",
       "    'a blurry picture of a television with a cat looking towards it',\n",
       "    'a tv with black curtain in a bathroom',\n",
       "    'this is an image of a close up of a speaker',\n",
       "    'looking onto a counter top that is also blurry',\n",
       "    'a close up of a mirror showing an abstract background',\n",
       "    'a piece of luggage with a clock in the background',\n",
       "    'there is a clock on the television set',\n",
       "    'a blurry photo of a small black cat',\n",
       "    'a large, round object in front of a blurry picture',\n",
       "    'a blurry photo of a television in a room',\n",
       "    'a large black microwave sitting next to a television'],\n",
       "   'captions_module_filter': ['an air cleaner sits in front of a mirror',\n",
       "    'view of a window with curtains with a blurry image in the background',\n",
       "    'very long exposure of someone standing in front of a window',\n",
       "    'a blurry picture of a tv screen in a living room',\n",
       "    'an blury photo of a tv set',\n",
       "    'a clock and window in a small room',\n",
       "    'black and white blurry photograph of a room with a clock',\n",
       "    'an odd blurry photograph of a tv and a television stand',\n",
       "    'a tv set sits on a wood table',\n",
       "    'person holding knife and knife holder, blurry with a window as background',\n",
       "    'blurry photograph of a plate of hotdogs and fries',\n",
       "    'a blurry image of a small dog next to a television set',\n",
       "    'the curtains that are closed in the room',\n",
       "    'there is a video game controller in the room',\n",
       "    'black sliding door seen from outside with blurred background',\n",
       "    'the reflection of a woman in black dresses in a room',\n",
       "    'a blurry picture of a television set in a living room',\n",
       "    'blurry shot of television with television in middle with chairs and a table in between it',\n",
       "    'a clock and television on wall near a large window',\n",
       "    'a luggage bag is next to the mirror',\n",
       "    'a close - up of black vases, and black glass',\n",
       "    'the window of a house is blurry and the tv monitor is turned on',\n",
       "    'a blurry picture of a door with a clock hanging on it',\n",
       "    'the television is on, against the wall as a blurry image',\n",
       "    'an indoor television set sitting next to a tree',\n",
       "    'a black cell phone in front of a door',\n",
       "    'a blurry photo of a chair and a mirror',\n",
       "    'there is a wine glass in the foreground',\n",
       "    'blurry view of a large living room with a television',\n",
       "    'blurry shot of bathroom showing toilet and sink'],\n",
       "   'label': 'Sneakers',\n",
       "   'location': 'lower middle',\n",
       "   'ratio': 0.001129570184275508,\n",
       "   'size': 'small',\n",
       "   'tag': ''},\n",
       "  {'attribute': '',\n",
       "   'box': [4.800379276275635,\n",
       "    37.91737747192383,\n",
       "    377.7641906738281,\n",
       "    296.6324157714844],\n",
       "   'captions_module': ['a person on a beach with a colorful kite',\n",
       "    'a man walking near water holding a kite',\n",
       "    'a man walks on the beach with a kite',\n",
       "    'a man with a kite at the beach with the ocean in the background',\n",
       "    'there is a man that is walking alone on the beach',\n",
       "    'a bird kite that is flying on the beach',\n",
       "    'a man is flying a kite by the beach',\n",
       "    'a man walks on the beach and flies a kite',\n",
       "    'a man holds onto a dragon tail kite along the beach',\n",
       "    'a man that is walking with a kite on a beach',\n",
       "    'a man on a beach flying a dragon kite',\n",
       "    'a man walks near a rainbow colored dragon kite',\n",
       "    'man flying a kite on the beach next to a body of water',\n",
       "    'a man in blue shirt holding a red long kite on the beach and walking to the water',\n",
       "    'a man is flying a kite on the beach',\n",
       "    'a man walking along the beach with a kite',\n",
       "    'a man trying to fly a kite at the beach',\n",
       "    'a man walks across a beach as a dragon kite approaches him',\n",
       "    'young man on beach holding kite on large beach',\n",
       "    'man walking on the beach with a kite',\n",
       "    'a man is walking along the beach as he flies a dragon kite',\n",
       "    'a person on a beach holding onto a colorful kite',\n",
       "    'a man holding onto a kite in a shape of a long kite',\n",
       "    'a man on a beach near the ocean making a dragon kite string tied to it and flying the kite',\n",
       "    'a man is walking with a kite on the beach',\n",
       "    'a man with a kite is on the beach',\n",
       "    'a man stands on the beach while flying a kite',\n",
       "    'a man is flying a kite near the water',\n",
       "    'a man walking on a beach with a kite',\n",
       "    'a colorful dragon kite is being flown on a beach'],\n",
       "   'captions_module_filter': ['a man preparing to launch a kite with a fish tail along the beach',\n",
       "    'man walking on the beach carrying a kite',\n",
       "    'the man is playing with the flying kite on the beach',\n",
       "    'a man standing on a beach in front of the ocean holding a kite string up for flying on the sand below the shore',\n",
       "    'this is a man flying a dragon kite on the beach',\n",
       "    'man walking on a beach holding a kite',\n",
       "    'a man is on the beach flying a kite',\n",
       "    'man flying and flying a colorful kite on the beach',\n",
       "    'man on the beach flying colorful kite',\n",
       "    'a man is flying a kite outside on the beach',\n",
       "    'man walking on the beach, holding a kite',\n",
       "    'a man holding a red dragon and flying a kite',\n",
       "    'the man is walking along the beach with his kite',\n",
       "    'man running a beach with a kite on his hand and the ocean beyond',\n",
       "    'a man is holding on to a kite flying near the water',\n",
       "    'a man is walking on the beach holding a kite which has a dragon tail',\n",
       "    'a man walking on the beach with a dragon head kite in hand',\n",
       "    'a man is pulling the string down the beach',\n",
       "    'a man walks with a kite on the wet sand',\n",
       "    'a man on a beach flying a kite',\n",
       "    'a man carrying a kite that has red string around it',\n",
       "    'a man on the beach running with a kite',\n",
       "    'a man walking down the beach with a kite',\n",
       "    'a guy pulling his kite along the beach',\n",
       "    'a man with a kite near a sea on a beach',\n",
       "    'a person walking along the beach with a dragon kite',\n",
       "    'the man is flying the kite on the beach',\n",
       "    'a man on a beach flying a kite next to the ocean',\n",
       "    'a man flying a dragon shaped kite at the beach',\n",
       "    'a man is standing next to the ocean with a kite on it'],\n",
       "   'label': 'Kite',\n",
       "   'location': 'middle',\n",
       "   'ratio': 0.5812731385231018,\n",
       "   'size': 'large',\n",
       "   'tag': ''},\n",
       "  {'attribute': '',\n",
       "   'box': [222.48748779296875,\n",
       "    36.33881759643555,\n",
       "    353.46234130859375,\n",
       "    205.1005401611328],\n",
       "   'captions_module': ['a large kite flying next to the ocean',\n",
       "    'a child and adult fly a blue and orange kite on a sand beach',\n",
       "    'a young girl running next to the sea on a beach on a nice winter day flying her dragon shaped kite',\n",
       "    'the kite in the shape of a dragon is being flown over the sand on the beach',\n",
       "    'a dragon like kite in flight next to the ocean',\n",
       "    'people by the ocean at shore with kites in the sand',\n",
       "    'one man is flying a kite on the beach',\n",
       "    'a dragon kite is flying in front of the water',\n",
       "    'a woman with a kite flying over a beach',\n",
       "    'a woman prepares to fly a large kite',\n",
       "    'a woman holding onto a colorful kite flying above the beach',\n",
       "    'a kid flying a big kite near the beach',\n",
       "    'a large kite flying in the sky over the ocean',\n",
       "    'the dragon kite flies low to the beach',\n",
       "    'man who is standing in the sand on a beach with a dragon kite',\n",
       "    'a man running down the beach with a kite',\n",
       "    'a woman walking with a kite on the beach',\n",
       "    'a young boy with a feather kite stands in the white surf of the beach beside the dark - haired dog',\n",
       "    'it looks kind of like it is playing near the ocean',\n",
       "    'two people on beach flying kites in the surf',\n",
       "    'a kite that appears to have wings painted out of red and blue sits on a beach, close to sea waves',\n",
       "    'a boy is flying his kite along the shoreline',\n",
       "    'a small child on the sand holding a kite',\n",
       "    'a young person carries a dragon kite down the beach',\n",
       "    'a boy in a wetsuit is flying his colorful kite',\n",
       "    'a woman is flying a dragon shaped kite',\n",
       "    'a girl with a dragon kite on the beach',\n",
       "    'a boy walking on the beach with a dragon kite',\n",
       "    'a small child flying a dragon kite on the beach in front of the ocean',\n",
       "    'a woman is running on the sand with a kite in the air'],\n",
       "   'captions_module_filter': ['two young people wearing dragon wings, flying',\n",
       "    'a child flying a dragon shaped kite on the beach',\n",
       "    'a kite is ready to fly, while a girl is barefoot in the sand',\n",
       "    'a child tries to fly a large dragon shaped kite',\n",
       "    'a person on the beach flying a kite',\n",
       "    'a kite that is being flown by the ocean',\n",
       "    'a picture of the young boy is flying a kite in the sand',\n",
       "    'a dog and a young man are playing with a kite',\n",
       "    'the big dragon kite is flying near the wave and the beach',\n",
       "    'a little kid is playing with a kite on the beach',\n",
       "    'a kid flying a kite with a red dragon tail',\n",
       "    'a dragon kite flies at the beach near the ocean',\n",
       "    'a young boy flying a dragon kite on the beach',\n",
       "    'a brown dog looking at a peacock kite by the ocean',\n",
       "    'a man flying a dragon kite on a beach',\n",
       "    'a small child running along the beach flying a dragon kite',\n",
       "    'a person on the beach flying a dragon kite',\n",
       "    'a child looking at a kite being flown by a person',\n",
       "    'colorful child at the beach getting ready to fly kite',\n",
       "    'a girl flying her kite on the beach',\n",
       "    'a colorful kite with wings on a beach',\n",
       "    'a boy flying a huge kite near the ocean',\n",
       "    'a person walks on a beach with a colorful bat kite',\n",
       "    'a child flying a kite on the beach with ocean waves',\n",
       "    'a person flying a orange dragon kite on the beach',\n",
       "    'a man with a kite standing on a beach',\n",
       "    'a kite that is flying at a beach',\n",
       "    'a person on the beach flying a colorful dragon kite',\n",
       "    'a dog walking on a beach next to a colorful bird kite',\n",
       "    'a young child with a dragon kite at a beach'],\n",
       "   'label': 'Kite',\n",
       "   'location': 'middle',\n",
       "   'ratio': 0.13315385580062866,\n",
       "   'size': 'small',\n",
       "   'tag': ''}]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview first sample\n",
    "samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "96f65b5039e74b789e042c8f086cf076",
      "1c02208ae3014312bdac3480bbaba754",
      "1e66d0c2d1924685b3b9849388976f16",
      "506335f15f2f466780b3243d7b31f5fd",
      "848eea9815814c2abdda91eb23c046ad",
      "035ad51cb6654f67a5516916b4953c7d",
      "73fa275c3b1e41f598393276c0a12a1f",
      "3a4c733821824ca1a442a8de1e363cb2",
      "545444cd30824736b7efdfc9590026f3",
      "fc9d146a819f45bfb5614a36900ff63f",
      "7a2e2d4116b64c698fd892fc92a697cd"
     ]
    },
    "id": "ckSCUHHtam_L",
    "outputId": "67119f98-e964-4d26-89ba-2f56a4d68438"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in local Jupyter notebook\n",
      "Device map: auto\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86f2bbdcfcb744e4bc074457f9d7d2ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n"
     ]
    }
   ],
   "source": [
    "# Check if running in Colab and set device accordingly\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    device_map = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(\"Running in Google Colab\")\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    device_map = \"auto\"\n",
    "    print(\"Running in local Jupyter notebook\")\n",
    "\n",
    "print(f\"Device map: {device_map}\")\n",
    "\n",
    "# Disable download progress bars to avoid widget errors\n",
    "# I guess it can be tested since it improves usability by a lot\n",
    "# try removing these lines and if a layout error appears, then put them back\n",
    "os.environ['HF_HUB_DISABLE_PROGRESS_BARS'] = '1'\n",
    "from huggingface_hub import utils as hf_utils\n",
    "hf_utils.disable_progress_bars()\n",
    "\n",
    "# Load the model on the available device(s)\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    dtype=\"auto\",\n",
    "    device_map=device_map\n",
    ")\n",
    "\n",
    "# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n",
    "# model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "#     \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     attn_implementation=\"flash_attention_2\",\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "# Re-enable progress bars for everything else (dataset loading, inference loops, etc.)\n",
    "del os.environ['HF_HUB_DISABLE_PROGRESS_BARS']\n",
    "hf_utils.enable_progress_bars()\n",
    "\n",
    "# default processor\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "## ONE OF THE EXPERIMENTS FOR VRAM USAGE ?\n",
    "# The default range for the number of visual tokens per image in the model is 4-16384. You can set min_pixels and max_pixels according to your needs, such as a token count range of 256-1280, to balance speed and memory usage.\n",
    "# min_pixels = 256*28*28\n",
    "# max_pixels = 1280*28*28\n",
    "#\n",
    "# processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JFtGMqYpCoSl",
    "outputId": "56174878-6b95-43de-b8aa-fa025858d716"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing VQAv2: 100%|██████████| 50/50 [00:19<00:00,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 50 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## ZERO-SHOT EVALUATION\n",
    "\n",
    "# Get the appropriate device (works for both Colab and local)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Process each sample\n",
    "results = []\n",
    "for idx, sample in enumerate(tqdm(samples, desc=\"Processing VQAv2\")):\n",
    "    # Prepare message for this sample\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": sample[\"image\"]},\n",
    "                {\"type\": \"text\", \"text\": sample[\"question\"]},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Preparation for inference\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "    # Inference: Generation of the output\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "    )\n",
    "\n",
    "    # Store results with all metadata needed for VQA evaluation\n",
    "    # Extract answer strings from the list of answer dictionaries\n",
    "    #ground_truth_answers = [ans['answer'] if isinstance(ans, dict) else ans for ans in sample[\"answers\"]]\n",
    "    \n",
    "    results.append({\n",
    "        \"question\": sample[\"question\"],\n",
    "        \"predicted_answer\": output_text[0],\n",
    "        \"ground_truth_answers\": sample[\"answers\"],\n",
    "        #\"ground_truth_answers\": ground_truth_answers,\n",
    "        \"question_type\": sample.get(\"question_type\", \"unknown\"),\n",
    "        \"answer_type\": sample.get(\"answer_type\", \"unknown\"),\n",
    "        \"question_id\": sample.get(\"question_id\", idx)\n",
    "    })\n",
    "\n",
    "print(f\"\\nProcessed {len(results)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4NElCabBjvyN"
   },
   "source": [
    "34min to precess 850 images (accidental keyboard interrupt) ~ around 4-5min each 100 samples\n",
    "\n",
    "on my (jacopo) italian pc: around 0.5s per sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rKOiFCqWj6Rg",
    "outputId": "6d6c90c1-8da4-411c-f71f-86b7a4912f87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample result:\n",
      "{'question': 'How many kites are in the picture?', 'predicted_answer': 'There is one kite in the picture.', 'ground_truth_answers': ['1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], 'question_type': 'how many', 'answer_type': 'number', 'question_id': 232309002}\n",
      "\n",
      "Ground truth answers structure:\n",
      "['1', '1', '1', '1', '1', '1', '1', '1', '1', '1']\n"
     ]
    }
   ],
   "source": [
    "# Check the structure of results\n",
    "print(\"Sample result:\")\n",
    "print(results[0])\n",
    "print(\"\\nGround truth answers structure:\")\n",
    "print(results[0]['ground_truth_answers'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X6sEX9yzqi3m"
   },
   "source": [
    "# backup code / stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "thO6POFYkr2R"
   },
   "outputs": [],
   "source": [
    "##############################################\n",
    "## EVALUATION FOR VQA\n",
    "## source: https://github.com/GT-Vision-Lab/VQA/blob/master/PythonEvaluationTools/vqaEvaluation/vqaEval.py\n",
    "##############################################\n",
    "# coding=utf-8\n",
    "\n",
    "import re\n",
    "# This code is based on the code written by Tsung-Yi Lin for MSCOCO Python API available at the following link:\n",
    "# (https://github.com/tylin/coco-caption/blob/master/pycocoevalcap/eval.py).\n",
    "import sys\n",
    "\n",
    "#####\n",
    "#\tclass for evaluating VQA results\n",
    "#\tparams: vqa: VQA object with ground truth annotations\n",
    "#\t\t\tvqaRes: VQA object with results to be evaluated\n",
    "#\t\t\tn: number of decimal places for accuracy\n",
    "#####\n",
    "class VQAEval:\n",
    "\tdef __init__(self, vqa, vqaRes, n=2):\n",
    "\t\t# Number of decimal places for rounding accuracy scores\n",
    "\t\tself.n \t\t\t  = n\n",
    "\t\t# Dictionary to store overall accuracy and breakdown by question/answer type\n",
    "\t\tself.accuracy     = {}\n",
    "\t\t# Dictionary to store per-question accuracy scores\n",
    "\t\tself.evalQA       = {}\n",
    "\t\t# Dictionary to store accuracy scores grouped by question type (e.g., \"what\", \"where\")\n",
    "\t\tself.evalQuesType = {}\n",
    "\t\t# Dictionary to store accuracy scores grouped by answer type (e.g., \"yes/no\", \"number\")\n",
    "\t\tself.evalAnsType  = {}\n",
    "\t\t# Ground truth VQA object containing correct annotations\n",
    "\t\tself.vqa \t\t  = vqa\n",
    "\t\t# Results VQA object containing model predictions to be evaluated\n",
    "\t\tself.vqaRes       = vqaRes\n",
    "\t\t# Parameters storing question IDs to evaluate\n",
    "\t\tself.params\t\t  = {'question_id': vqa.getQuesIds()}\n",
    "\n",
    "\t\t# Dictionary mapping contracted forms to their expanded versions\n",
    "\t\t# Used to normalize answers like \"cant\" -> \"can't\"\n",
    "\t\tself.contractions = {\"aint\": \"ain't\", \"arent\": \"aren't\", \"cant\": \"can't\", \"couldve\": \"could've\", \"couldnt\": \"couldn't\", \\\n",
    "\t\t\t\t\t\t\t \"couldn'tve\": \"couldn't've\", \"couldnt've\": \"couldn't've\", \"didnt\": \"didn't\", \"doesnt\": \"doesn't\", \"dont\": \"don't\", \"hadnt\": \"hadn't\", \\\n",
    "\t\t\t\t\t\t\t \"hadnt've\": \"hadn't've\", \"hadn'tve\": \"hadn't've\", \"hasnt\": \"hasn't\", \"havent\": \"haven't\", \"hed\": \"he'd\", \"hed've\": \"he'd've\", \\\n",
    "\t\t\t\t\t\t\t \"he'dve\": \"he'd've\", \"hes\": \"he's\", \"howd\": \"how'd\", \"howll\": \"how'll\", \"hows\": \"how's\", \"Id've\": \"I'd've\", \"I'dve\": \"I'd've\", \\\n",
    "\t\t\t\t\t\t\t \"Im\": \"I'm\", \"Ive\": \"I've\", \"isnt\": \"isn't\", \"itd\": \"it'd\", \"itd've\": \"it'd've\", \"it'dve\": \"it'd've\", \"itll\": \"it'll\", \"let's\": \"let's\", \\\n",
    "\t\t\t\t\t\t\t \"maam\": \"ma'am\", \"mightnt\": \"mightn't\", \"mightnt've\": \"mightn't've\", \"mightn'tve\": \"mightn't've\", \"mightve\": \"might've\", \\\n",
    "\t\t\t\t\t\t\t \"mustnt\": \"mustn't\", \"mustve\": \"must've\", \"neednt\": \"needn't\", \"notve\": \"not've\", \"oclock\": \"o'clock\", \"oughtnt\": \"oughtn't\", \\\n",
    "\t\t\t\t\t\t\t \"ow's'at\": \"'ow's'at\", \"'ows'at\": \"'ow's'at\", \"'ow'sat\": \"'ow's'at\", \"shant\": \"shan't\", \"shed've\": \"she'd've\", \"she'dve\": \"she'd've\", \\\n",
    "\t\t\t\t\t\t\t \"she's\": \"she's\", \"shouldve\": \"should've\", \"shouldnt\": \"shouldn't\", \"shouldnt've\": \"shouldn't've\", \"shouldn'tve\": \"shouldn't've\", \\\n",
    "\t\t\t\t\t\t\t \"somebody'd\": \"somebodyd\", \"somebodyd've\": \"somebody'd've\", \"somebody'dve\": \"somebody'd've\", \"somebodyll\": \"somebody'll\", \\\n",
    "\t\t\t\t\t\t\t \"somebodys\": \"somebody's\", \"someoned\": \"someone'd\", \"someoned've\": \"someone'd've\", \"someone'dve\": \"someone'd've\", \\\n",
    "\t\t\t\t\t\t\t \"someonell\": \"someone'll\", \"someones\": \"someone's\", \"somethingd\": \"something'd\", \"somethingd've\": \"something'd've\", \\\n",
    "\t\t\t\t\t\t\t \"something'dve\": \"something'd've\", \"somethingll\": \"something'll\", \"thats\": \"that's\", \"thered\": \"there'd\", \"thered've\": \"there'd've\", \\\n",
    "\t\t\t\t\t\t\t \"there'dve\": \"there'd've\", \"therere\": \"there're\", \"theres\": \"there's\", \"theyd\": \"they'd\", \"theyd've\": \"they'd've\", \\\n",
    "\t\t\t\t\t\t\t \"they'dve\": \"they'd've\", \"theyll\": \"they'll\", \"theyre\": \"they're\", \"theyve\": \"they've\", \"twas\": \"'twas\", \"wasnt\": \"wasn't\", \\\n",
    "\t\t\t\t\t\t\t \"wed've\": \"we'd've\", \"we'dve\": \"we'd've\", \"weve\": \"we've\", \"werent\": \"weren't\", \"whatll\": \"what'll\", \"whatre\": \"what're\", \\\n",
    "\t\t\t\t\t\t\t \"whats\": \"what's\", \"whatve\": \"what've\", \"whens\": \"when's\", \"whered\": \"where'd\", \"wheres\": \"where's\", \"whereve\": \"where've\", \\\n",
    "\t\t\t\t\t\t\t \"whod\": \"who'd\", \"whod've\": \"who'd've\", \"who'dve\": \"who'd've\", \"wholl\": \"who'll\", \"whos\": \"who's\", \"whove\": \"who've\", \"whyll\": \"why'll\", \\\n",
    "\t\t\t\t\t\t\t \"whyre\": \"why're\", \"whys\": \"why's\", \"wont\": \"won't\", \"wouldve\": \"would've\", \"wouldnt\": \"wouldn't\", \"wouldnt've\": \"wouldn't've\", \\\n",
    "\t\t\t\t\t\t\t \"wouldn'tve\": \"wouldn't've\", \"yall\": \"y'all\", \"yall'll\": \"y'all'll\", \"y'allll\": \"y'all'll\", \"yall'd've\": \"y'all'd've\", \\\n",
    "\t\t\t\t\t\t\t \"y'alld've\": \"y'all'd've\", \"y'all'dve\": \"y'all'd've\", \"youd\": \"you'd\", \"youd've\": \"you'd've\", \"you'dve\": \"you'd've\", \\\n",
    "\t\t\t\t\t\t\t \"youll\": \"you'll\", \"youre\": \"you're\", \"youve\": \"you've\"}\n",
    "\n",
    "\t\t# Dictionary to convert number words to digit strings\n",
    "\t\t# Ensures \"three\" and \"3\" are treated as equivalent\n",
    "\t\tself.manualMap    = { 'none': '0',\n",
    "\t\t\t\t\t\t\t  'zero': '0',\n",
    "\t\t\t\t\t\t\t  'one': '1',\n",
    "\t\t\t\t\t\t\t  'two': '2',\n",
    "\t\t\t\t\t\t\t  'three': '3',\n",
    "\t\t\t\t\t\t\t  'four': '4',\n",
    "\t\t\t\t\t\t\t  'five': '5',\n",
    "\t\t\t\t\t\t\t  'six': '6',\n",
    "\t\t\t\t\t\t\t  'seven': '7',\n",
    "\t\t\t\t\t\t\t  'eight': '8',\n",
    "\t\t\t\t\t\t\t  'nine': '9',\n",
    "\t\t\t\t\t\t\t  'ten': '10'\n",
    "\t\t\t\t\t\t\t}\n",
    "\n",
    "\t\t# List of articles to be removed during normalization\n",
    "\t\t# Ensures \"the dog\" and \"dog\" are treated as equivalent\n",
    "\t\tself.articles     = ['a',\n",
    "\t\t\t\t\t\t\t 'an',\n",
    "\t\t\t\t\t\t\t 'the'\n",
    "\t\t\t\t\t\t\t]\n",
    "\n",
    "\t\t# Regex pattern to remove periods that are not part of decimal numbers\n",
    "\t\tself.periodStrip  = re.compile(r\"(?!<=\\d)(\\.)(?!\\d)\")\n",
    "\t\t# Regex pattern to handle commas in numbers (e.g., \"1,000\")\n",
    "\t\tself.commaStrip   = re.compile(r\"(\\d)(\\,)(\\d)\")\n",
    "\t\t# List of punctuation marks to be normalized or removed\n",
    "\t\tself.punct        = [';', r\"/\", '[', ']', '\"', '{', '}',\n",
    "\t\t\t\t\t\t\t '(', ')', '=', '+', '\\\\', '_', '-',\n",
    "\t\t\t\t\t\t\t '>', '<', '@', '`', ',', '?', '!']\n",
    "\n",
    "\n",
    "\tdef evaluate(self, quesIds=None):\n",
    "\t\t# If no specific question IDs provided, evaluate all questions\n",
    "\t\tif quesIds == None:\n",
    "\t\t\tquesIds = [quesId for quesId in self.params['question_id']]\n",
    "\n",
    "\t\t# Create dictionaries to hold ground truth and result answers for each question\n",
    "\t\tgts = {}\n",
    "\t\tres = {}\n",
    "\t\tfor quesId in quesIds:\n",
    "\t\t\tgts[quesId] = self.vqa.qa[quesId]\n",
    "\t\t\tres[quesId] = self.vqaRes.qa[quesId]\n",
    "\n",
    "\t\t# =================================================\n",
    "\t\t# Compute accuracy\n",
    "\t\t# =================================================\n",
    "\t\t# List to store accuracy scores for all questions\n",
    "\t\taccQA       = []\n",
    "\t\t# Dictionary to accumulate accuracy scores by question type\n",
    "\t\taccQuesType = {}\n",
    "\t\t# Dictionary to accumulate accuracy scores by answer type\n",
    "\t\taccAnsType  = {}\n",
    "\t\tprint(\"computing accuracy\")\n",
    "\t\tstep = 0\n",
    "\n",
    "\t\t# Iterate through each question to compute accuracy\n",
    "\t\tfor quesId in quesIds:\n",
    "\t\t\t# Clean ground truth answers: replace newlines and tabs with spaces, strip whitespace\n",
    "\t\t\tfor ansDic in gts[quesId]['answers']:\n",
    "\t\t\t\tansDic['answer'] = ansDic['answer'].replace('\\n', ' ')\n",
    "\t\t\t\tansDic['answer'] = ansDic['answer'].replace('\\t', ' ')\n",
    "\t\t\t\tansDic['answer'] = ansDic['answer'].strip()\n",
    "\n",
    "\t\t\t# Clean the predicted answer in the same way\n",
    "\t\t\tresAns = res[quesId]['answer']\n",
    "\t\t\tresAns = resAns.replace('\\n', ' ')\n",
    "\t\t\tresAns = resAns.replace('\\t', ' ')\n",
    "\t\t\tresAns = resAns.strip()\n",
    "\n",
    "\t\t\t# List to store accuracy scores when comparing prediction to each ground truth answer\n",
    "\t\t\tgtAcc = []\n",
    "\t\t\t# Extract all ground truth answer texts\n",
    "\t\t\tgtAnswers = [ans['answer'] for ans in gts[quesId]['answers']]\n",
    "\n",
    "\t\t\t# Only apply normalization if there are multiple different ground truth answers\n",
    "\t\t\t# This preserves exact matches when all annotators agree\n",
    "\t\t\tif len(set(gtAnswers)) > 1:\n",
    "\t\t\t\t# Normalize ground truth answers: process punctuation and convert digits/articles\n",
    "\t\t\t\tfor ansDic in gts[quesId]['answers']:\n",
    "\t\t\t\t\tansDic['answer'] = self.processPunctuation(ansDic['answer'])\n",
    "\t\t\t\t\tansDic['answer'] = self.processDigitArticle(ansDic['answer'])\n",
    "\t\t\t\t# Normalize the predicted answer in the same way\n",
    "\t\t\t\tresAns = self.processPunctuation(resAns)\n",
    "\t\t\t\tresAns = self.processDigitArticle(resAns)\n",
    "\n",
    "\t\t\t# VQA accuracy metric: for each ground truth answer, check how many OTHER annotators\n",
    "\t\t\t# gave the same answer as the prediction\n",
    "\t\t\tfor gtAnsDatum in gts[quesId]['answers']:\n",
    "\t\t\t\t# Get all OTHER ground truth answers (excluding current one)\n",
    "\t\t\t\totherGTAns = [item for item in gts[quesId]['answers'] if item!=gtAnsDatum]\n",
    "\t\t\t\t# Count how many of the other annotators gave the same answer as prediction\n",
    "\t\t\t\tmatchingAns = [item for item in otherGTAns if item['answer']==resAns]\n",
    "\t\t\t\t# Accuracy is min(1, matching_count/3)\n",
    "\t\t\t\t# If 3+ annotators agree with prediction -> 100% accuracy\n",
    "\t\t\t\t# If 2 annotators agree -> 66.7% accuracy\n",
    "\t\t\t\t# If 1 annotator agrees -> 33.3% accuracy\n",
    "\t\t\t\t# If 0 annotators agree -> 0% accuracy\n",
    "\t\t\t\tacc = min(1, float(len(matchingAns))/3)\n",
    "\t\t\t\tgtAcc.append(acc)\n",
    "\n",
    "\t\t\t# Get metadata for this question\n",
    "\t\t\tquesType    = gts[quesId]['question_type']\n",
    "\t\t\tansType     = gts[quesId]['answer_type']\n",
    "\n",
    "\t\t\t# Average the accuracy across all ground truth answers (usually 10 answers per question)\n",
    "\t\t\tavgGTAcc = float(sum(gtAcc))/len(gtAcc)\n",
    "\n",
    "\t\t\t# Store this question's accuracy in the overall list\n",
    "\t\t\taccQA.append(avgGTAcc)\n",
    "\n",
    "\t\t\t# Accumulate accuracy by question type (e.g., \"what color\", \"how many\")\n",
    "\t\t\tif quesType not in accQuesType:\n",
    "\t\t\t\taccQuesType[quesType] = []\n",
    "\t\t\taccQuesType[quesType].append(avgGTAcc)\n",
    "\n",
    "\t\t\t# Accumulate accuracy by answer type (e.g., \"yes/no\", \"number\", \"other\")\n",
    "\t\t\tif ansType not in accAnsType:\n",
    "\t\t\t\taccAnsType[ansType] = []\n",
    "\t\t\taccAnsType[ansType].append(avgGTAcc)\n",
    "\n",
    "\t\t\t# Store individual question accuracy\n",
    "\t\t\tself.setEvalQA(quesId, avgGTAcc)\n",
    "\t\t\t# Store accuracy grouped by question type\n",
    "\t\t\tself.setEvalQuesType(quesId, quesType, avgGTAcc)\n",
    "\t\t\t# Store accuracy grouped by answer type\n",
    "\t\t\tself.setEvalAnsType(quesId, ansType, avgGTAcc)\n",
    "\n",
    "\t\t\t# Update progress bar every 100 questions\n",
    "\t\t\tif step%100 == 0:\n",
    "\t\t\t\tself.updateProgress(step/float(len(quesIds)))\n",
    "\t\t\tstep = step + 1\n",
    "\n",
    "\t\t# Calculate final accuracy metrics (overall and per type)\n",
    "\t\tself.setAccuracy(accQA, accQuesType, accAnsType)\n",
    "\t\tprint(\"Done computing accuracy\")\n",
    "\n",
    "\tdef processPunctuation(self, inText):\n",
    "\t\t# Process punctuation in text for normalization\n",
    "\t\toutText = inText\n",
    "\t\tfor p in self.punct:\n",
    "\t\t\t# If punctuation is surrounded by spaces or appears in comma-separated numbers\n",
    "\t\t\tif (p + ' ' in inText or ' ' + p in inText) or (re.search(self.commaStrip, inText) != None):\n",
    "\t\t\t\t# Remove the punctuation completely\n",
    "\t\t\t\toutText = outText.replace(p, '')\n",
    "\t\t\telse:\n",
    "\t\t\t\t# Otherwise replace with space to separate words\n",
    "\t\t\t\toutText = outText.replace(p, ' ')\n",
    "\t\t# Remove periods that are not part of decimal numbers\n",
    "\t\toutText = self.periodStrip.sub(\"\",\n",
    "\t\t\t\t\t\t\t\t\t  outText,\n",
    "\t\t\t\t\t\t\t\t\t  re.UNICODE)\n",
    "\t\treturn outText\n",
    "\n",
    "\tdef processDigitArticle(self, inText):\n",
    "\t\t# Process text to normalize digits, remove articles, and expand contractions\n",
    "\t\toutText = []\n",
    "\t\t# Convert to lowercase and split into words\n",
    "\t\ttempText = inText.lower().split()\n",
    "\t\tfor word in tempText:\n",
    "\t\t\t# Convert number words to digits (e.g., \"three\" -> \"3\")\n",
    "\t\t\t# If word not in manualMap, keep it unchanged\n",
    "\t\t\tword = self.manualMap.setdefault(word, word)\n",
    "\t\t\t# Remove articles (\"a\", \"an\", \"the\")\n",
    "\t\t\tif word not in self.articles:\n",
    "\t\t\t\toutText.append(word)\n",
    "\t\t\telse:\n",
    "\t\t\t\tpass\n",
    "\t\t# Expand contractions (e.g., \"can't\" -> \"cannot\")\n",
    "\t\tfor wordId, word in enumerate(outText):\n",
    "\t\t\tif word in self.contractions:\n",
    "\t\t\t\toutText[wordId] = self.contractions[word]\n",
    "\t\t# Join words back into a single string\n",
    "\t\toutText = ' '.join(outText)\n",
    "\t\treturn outText\n",
    "\n",
    "\tdef setAccuracy(self, accQA, accQuesType, accAnsType):\n",
    "\t\t# Calculate and store overall accuracy as percentage\n",
    "\t\tself.accuracy['overall']         = round(100*float(sum(accQA))/len(accQA), self.n)\n",
    "\t\t# Calculate and store average accuracy for each question type\n",
    "\t\tself.accuracy['perQuestionType'] = {quesType: round(100*float(sum(accQuesType[quesType]))/len(accQuesType[quesType]), self.n) for quesType in accQuesType}\n",
    "\t\t# Calculate and store average accuracy for each answer type\n",
    "\t\tself.accuracy['perAnswerType']   = {ansType:  round(100*float(sum(accAnsType[ansType]))/len(accAnsType[ansType]), self.n) for ansType in accAnsType}\n",
    "\n",
    "\tdef setEvalQA(self, quesId, acc):\n",
    "\t\t# Store accuracy for a specific question (converted to percentage)\n",
    "\t\tself.evalQA[quesId] = round(100*acc, self.n)\n",
    "\n",
    "\tdef setEvalQuesType(self, quesId, quesType, acc):\n",
    "\t\t# Store accuracy for a question grouped by its question type\n",
    "\t\tif quesType not in self.evalQuesType:\n",
    "\t\t\tself.evalQuesType[quesType] = {}\n",
    "\t\tself.evalQuesType[quesType][quesId] = round(100*acc, self.n)\n",
    "\n",
    "\tdef setEvalAnsType(self, quesId, ansType, acc):\n",
    "\t\t# Store accuracy for a question grouped by its answer type\n",
    "\t\tif ansType not in self.evalAnsType:\n",
    "\t\t\tself.evalAnsType[ansType] = {}\n",
    "\t\tself.evalAnsType[ansType][quesId] = round(100*acc, self.n)\n",
    "\n",
    "\tdef updateProgress(self, progress):\n",
    "\t\t# Display a text-based progress bar in the terminal\n",
    "\t\tbarLength = 20\n",
    "\t\tstatus = \"\"\n",
    "\t\t# Convert integer to float if needed\n",
    "\t\tif isinstance(progress, int):\n",
    "\t\t\tprogress = float(progress)\n",
    "\t\t# Validate progress value\n",
    "\t\tif not isinstance(progress, float):\n",
    "\t\t\tprogress = 0\n",
    "\t\t\tstatus = \"error: progress var must be float\\r\\n\"\n",
    "\t\tif progress < 0:\n",
    "\t\t\tprogress = 0\n",
    "\t\t\tstatus = \"Halt...\\r\\n\"\n",
    "\t\tif progress >= 1:\n",
    "\t\t\tprogress = 1\n",
    "\t\t\tstatus = \"Done...\\r\\n\"\n",
    "\t\t# Calculate how many blocks to fill in progress bar\n",
    "\t\tblock = int(round(barLength*progress))\n",
    "\t\t# Format progress bar: [####------------] 40%\n",
    "\t\ttext = \"\\rFinshed Percent: [{0}] {1}% {2}\".format( \"#\"*block + \"-\"*(barLength-block), int(progress*100), status)\n",
    "\t\tsys.stdout.write(text)\n",
    "\t\tsys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "LZ6ySvz_wObY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating VQA evaluation objects...\n",
      "Running VQA evaluation...\n",
      "computing accuracy\n",
      "Finshed Percent: [--------------------] 0% Done computing accuracy\n",
      "\n",
      "============================================================\n",
      "VQA EVALUATION RESULTS\n",
      "============================================================\n",
      "Overall Accuracy: 0.00%\n",
      "\n",
      "Evaluated 50 questions\n",
      "\n",
      "------------------------------------------------------------\n",
      "Accuracy by Question Type:\n",
      "------------------------------------------------------------\n",
      "  are the                       : 0.00%\n",
      "  are these                     : 0.00%\n",
      "  are they                      : 0.00%\n",
      "  can you                       : 0.00%\n",
      "  could                         : 0.00%\n",
      "  does this                     : 0.00%\n",
      "  has                           : 0.00%\n",
      "  how                           : 0.00%\n",
      "  how many                      : 0.00%\n",
      "  how many people are           : 0.00%\n",
      "  is                            : 0.00%\n",
      "  is that a                     : 0.00%\n",
      "  is the                        : 0.00%\n",
      "  is the man                    : 0.00%\n",
      "  is there                      : 0.00%\n",
      "  is there a                    : 0.00%\n",
      "  is this                       : 0.00%\n",
      "  is this a                     : 0.00%\n",
      "  what                          : 0.00%\n",
      "  what are                      : 0.00%\n",
      "  what are the                  : 0.00%\n",
      "  what brand                    : 0.00%\n",
      "  what color is the             : 0.00%\n",
      "  what does the                 : 0.00%\n",
      "  what is                       : 0.00%\n",
      "  what is in the                : 0.00%\n",
      "  what is the                   : 0.00%\n",
      "  what is the color of the      : 0.00%\n",
      "  what is the man               : 0.00%\n",
      "  where is the                  : 0.00%\n",
      "\n",
      "------------------------------------------------------------\n",
      "Accuracy by Answer Type:\n",
      "------------------------------------------------------------\n",
      "  number                        : 0.00%\n",
      "  other                         : 0.00%\n",
      "  yes/no                        : 0.00%\n",
      "============================================================\n",
      "Done computing accuracy\n",
      "\n",
      "============================================================\n",
      "VQA EVALUATION RESULTS\n",
      "============================================================\n",
      "Overall Accuracy: 0.00%\n",
      "\n",
      "Evaluated 50 questions\n",
      "\n",
      "------------------------------------------------------------\n",
      "Accuracy by Question Type:\n",
      "------------------------------------------------------------\n",
      "  are the                       : 0.00%\n",
      "  are these                     : 0.00%\n",
      "  are they                      : 0.00%\n",
      "  can you                       : 0.00%\n",
      "  could                         : 0.00%\n",
      "  does this                     : 0.00%\n",
      "  has                           : 0.00%\n",
      "  how                           : 0.00%\n",
      "  how many                      : 0.00%\n",
      "  how many people are           : 0.00%\n",
      "  is                            : 0.00%\n",
      "  is that a                     : 0.00%\n",
      "  is the                        : 0.00%\n",
      "  is the man                    : 0.00%\n",
      "  is there                      : 0.00%\n",
      "  is there a                    : 0.00%\n",
      "  is this                       : 0.00%\n",
      "  is this a                     : 0.00%\n",
      "  what                          : 0.00%\n",
      "  what are                      : 0.00%\n",
      "  what are the                  : 0.00%\n",
      "  what brand                    : 0.00%\n",
      "  what color is the             : 0.00%\n",
      "  what does the                 : 0.00%\n",
      "  what is                       : 0.00%\n",
      "  what is in the                : 0.00%\n",
      "  what is the                   : 0.00%\n",
      "  what is the color of the      : 0.00%\n",
      "  what is the man               : 0.00%\n",
      "  where is the                  : 0.00%\n",
      "\n",
      "------------------------------------------------------------\n",
      "Accuracy by Answer Type:\n",
      "------------------------------------------------------------\n",
      "  number                        : 0.00%\n",
      "  other                         : 0.00%\n",
      "  yes/no                        : 0.00%\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "##############################################\n",
    "## WRAPPER TO USE VQAEval WITH RESULTS ARRAY\n",
    "##############################################\n",
    "\n",
    "# Create wrapper classes that adapt our results format to VQAEval's expected format\n",
    "class SimpleVQA:\n",
    "    \"\"\"Wrapper for ground truth data - preserves all metadata\"\"\"\n",
    "    def __init__(self, results):\n",
    "        self.qa = {}\n",
    "        for idx, result in enumerate(results):\n",
    "            # Use question_id from dataset if available, otherwise use index\n",
    "            qid = result.get('question_id', idx)\n",
    "            self.qa[qid] = {\n",
    "                'question': result['question'],\n",
    "                'answers': [{'answer': ans} for ans in result['ground_truth_answers']],\n",
    "                'question_type': result.get('question_type', 'unknown'),\n",
    "                'answer_type': result.get('answer_type', 'unknown')\n",
    "            }\n",
    "\n",
    "    def getQuesIds(self):\n",
    "        return list(self.qa.keys())\n",
    "\n",
    "class SimpleVQARes:\n",
    "    \"\"\"Wrapper for predicted results\"\"\"\n",
    "    def __init__(self, results):\n",
    "        self.qa = {}\n",
    "        for idx, result in enumerate(results):\n",
    "            # Use question_id from dataset if available, otherwise use index\n",
    "            qid = result.get('question_id', idx)\n",
    "            self.qa[qid] = {\n",
    "                'answer': result['predicted_answer']\n",
    "            }\n",
    "\n",
    "# Create VQA objects from your results\n",
    "print(\"Creating VQA evaluation objects...\")\n",
    "vqa = SimpleVQA(results)\n",
    "vqaRes = SimpleVQARes(results)\n",
    "\n",
    "# Run evaluation\n",
    "print(\"Running VQA evaluation...\")\n",
    "vqaEval = VQAEval(vqa, vqaRes, n=2)\n",
    "vqaEval.evaluate()\n",
    "\n",
    "# Print results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VQA EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Overall Accuracy: {vqaEval.accuracy['overall']:.2f}%\")\n",
    "print(f\"\\nEvaluated {len(results)} questions\")\n",
    "\n",
    "# Print per question type accuracy if available\n",
    "if vqaEval.accuracy['perQuestionType']:\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"Accuracy by Question Type:\")\n",
    "    print(\"-\"*60)\n",
    "    for qtype, acc in sorted(vqaEval.accuracy['perQuestionType'].items()):\n",
    "        print(f\"  {qtype:30s}: {acc:.2f}%\")\n",
    "\n",
    "# Print per answer type accuracy if available\n",
    "if vqaEval.accuracy['perAnswerType']:\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"Accuracy by Answer Type:\")\n",
    "    print(\"-\"*60)\n",
    "    for atype, acc in sorted(vqaEval.accuracy['perAnswerType'].items()):\n",
    "        print(f\"  {atype:30s}: {acc:.2f}%\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DEBUGGING EVALUATION ===\n",
      "\n",
      "First result:\n",
      "Question: How many kites are in the picture?\n",
      "Predicted: 'There is one kite in the picture.'\n",
      "Ground truth: ['1', '1', '1', '1', '1', '1', '1', '1', '1', '1']\n",
      "\n",
      "Question type: how many\n",
      "Answer type: number\n",
      "\n",
      "=== WRAPPER OUTPUT ===\n",
      "Question ID: 232309002\n",
      "Ground truth structure: {'question': 'How many kites are in the picture?', 'answers': [{'answer': '1'}, {'answer': '1'}, {'answer': '1'}, {'answer': '1'}, {'answer': '1'}, {'answer': '1'}, {'answer': '1'}, {'answer': '1'}, {'answer': '1'}, {'answer': '1'}], 'question_type': 'how many', 'answer_type': 'number'}\n",
      "Prediction structure: {'answer': 'There is one kite in the picture.'}\n",
      "\n",
      "=== INDIVIDUAL QUESTION ACCURACY ===\n",
      "Accuracy for question ID 232309002: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Detailed debugging specific example\n",
    "print(\"=== DEBUGGING EVALUATION ===\\n\")\n",
    "print(\"First result:\")\n",
    "print(f\"Question: {results[0]['question']}\")\n",
    "print(f\"Predicted: '{results[0]['predicted_answer']}'\")\n",
    "print(f\"Ground truth: {results[0]['ground_truth_answers']}\")\n",
    "print(f\"\\nQuestion type: {results[0]['question_type']}\")\n",
    "print(f\"Answer type: {results[0]['answer_type']}\")\n",
    "\n",
    "# Check what the wrapper creates\n",
    "print(\"\\n=== WRAPPER OUTPUT ===\")\n",
    "vqa_test = SimpleVQA(results[:1])\n",
    "vqaRes_test = SimpleVQARes(results[:1])\n",
    "qid = list(vqa_test.qa.keys())[0]\n",
    "print(f\"Question ID: {qid}\")\n",
    "print(f\"Ground truth structure: {vqa_test.qa[qid]}\")\n",
    "print(f\"Prediction structure: {vqaRes_test.qa[qid]}\")\n",
    "\n",
    "print(\"\\n=== INDIVIDUAL QUESTION ACCURACY ===\")\n",
    "quesId = results[0].get('question_id', 0)\n",
    "acc = vqaEval.evalQA[quesId]\n",
    "print(f\"Accuracy for question ID {quesId}: {acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MANUAL EVALUATION TRACE ===\n",
      "\n",
      "Question: How many kites are in the picture?\n",
      "Predicted: 'There is one kite in the picture.'\n",
      "Ground truth: ['1', '1', '1', '1', '1', '1', '1', '1', '1', '1']\n",
      "\n",
      "=== AFTER WRAPPER ===\n",
      "Ground truth answers: [{'answer': '1'}, {'answer': '1'}, {'answer': '1'}, {'answer': '1'}, {'answer': '1'}, {'answer': '1'}, {'answer': '1'}, {'answer': '1'}, {'answer': '1'}, {'answer': '1'}]\n",
      "Predicted answer: 'There is one kite in the picture.'\n",
      "\n",
      "=== VQA SCORING LOGIC ===\n",
      "Cleaned prediction: 'There is one kite in the picture.'\n",
      "All GT answers: ['1', '1', '1', '1', '1', '1', '1', '1', '1', '1']\n",
      "Unique GT answers: {'1'}\n",
      "\n",
      "For first GT answer '1':\n",
      "Other GT answers: []\n",
      "Matching answers: []\n",
      "Match count: 0\n",
      "Accuracy contribution: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Manual trace through evaluation logic for first question\n",
    "print(\"=== MANUAL EVALUATION TRACE ===\\n\")\n",
    "\n",
    "# Get first result\n",
    "result = results[0]\n",
    "print(f\"Question: {result['question']}\")\n",
    "print(f\"Predicted: '{result['predicted_answer']}'\")\n",
    "print(f\"Ground truth: {result['ground_truth_answers']}\")\n",
    "\n",
    "# Simulate what VQAEval does\n",
    "qid = result.get('question_id', 0)\n",
    "vqa_test = SimpleVQA([result])\n",
    "vqaRes_test = SimpleVQARes([result])\n",
    "\n",
    "gts = vqa_test.qa[qid]\n",
    "res = vqaRes_test.qa[qid]\n",
    "\n",
    "print(f\"\\n=== AFTER WRAPPER ===\")\n",
    "print(f\"Ground truth answers: {gts['answers']}\")\n",
    "print(f\"Predicted answer: '{res['answer']}'\")\n",
    "\n",
    "# Simulate VQA evaluation logic\n",
    "resAns = res['answer'].strip()\n",
    "gtAnswers = [ans['answer'] for ans in gts['answers']]\n",
    "\n",
    "print(f\"\\n=== VQA SCORING LOGIC ===\")\n",
    "print(f\"Cleaned prediction: '{resAns}'\")\n",
    "print(f\"All GT answers: {gtAnswers}\")\n",
    "print(f\"Unique GT answers: {set(gtAnswers)}\")\n",
    "\n",
    "# Check matches for first GT answer\n",
    "gtAnsDatum = gts['answers'][0]\n",
    "otherGTAns = [item for item in gts['answers'] if item!=gtAnsDatum]\n",
    "print(f\"\\nFor first GT answer '{gtAnsDatum['answer']}':\")\n",
    "print(f\"Other GT answers: {[item['answer'] for item in otherGTAns]}\")\n",
    "\n",
    "matchingAns = [item for item in otherGTAns if item['answer']==resAns]\n",
    "print(f\"Matching answers: {[item['answer'] for item in matchingAns]}\")\n",
    "print(f\"Match count: {len(matchingAns)}\")\n",
    "print(f\"Accuracy contribution: {min(1, len(matchingAns)/3)}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "035ad51cb6654f67a5516916b4953c7d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "076c0d24260f4f18a43a9558ce664cb2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2c22bae211bb4b3ab440c1ee987dd544",
       "IPY_MODEL_aee2363983ee4647a2aff35edb6f35d1",
       "IPY_MODEL_67a58360f1fe4789be2e9ab67695a3dc"
      ],
      "layout": "IPY_MODEL_f32519912fa34ff99ff31367cae10338"
     }
    },
    "0826cf8dd39b493c9679db0cab2a9674": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1c02208ae3014312bdac3480bbaba754": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_035ad51cb6654f67a5516916b4953c7d",
      "placeholder": "​",
      "style": "IPY_MODEL_73fa275c3b1e41f598393276c0a12a1f",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "1e66d0c2d1924685b3b9849388976f16": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3a4c733821824ca1a442a8de1e363cb2",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_545444cd30824736b7efdfc9590026f3",
      "value": 2
     }
    },
    "2c22bae211bb4b3ab440c1ee987dd544": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d84382de4ad64141b92859057bda4b4a",
      "placeholder": "​",
      "style": "IPY_MODEL_7ffe2e595eea48ac81995d3c71c1af38",
      "value": "Resolving data files: 100%"
     }
    },
    "3a4c733821824ca1a442a8de1e363cb2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "506335f15f2f466780b3243d7b31f5fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fc9d146a819f45bfb5614a36900ff63f",
      "placeholder": "​",
      "style": "IPY_MODEL_7a2e2d4116b64c698fd892fc92a697cd",
      "value": " 2/2 [00:22&lt;00:00,  9.49s/it]"
     }
    },
    "545444cd30824736b7efdfc9590026f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "668bcd14eacf47638aecee5a3fe71330": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "67a58360f1fe4789be2e9ab67695a3dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d37b98ed92e1493091ba99c3acea4660",
      "placeholder": "​",
      "style": "IPY_MODEL_8910c01866dd4650a5bab066f517d96e",
      "value": " 90/90 [00:00&lt;00:00, 1610.22it/s]"
     }
    },
    "73fa275c3b1e41f598393276c0a12a1f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7a2e2d4116b64c698fd892fc92a697cd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7ffe2e595eea48ac81995d3c71c1af38": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "848eea9815814c2abdda91eb23c046ad": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8910c01866dd4650a5bab066f517d96e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "96f65b5039e74b789e042c8f086cf076": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1c02208ae3014312bdac3480bbaba754",
       "IPY_MODEL_1e66d0c2d1924685b3b9849388976f16",
       "IPY_MODEL_506335f15f2f466780b3243d7b31f5fd"
      ],
      "layout": "IPY_MODEL_848eea9815814c2abdda91eb23c046ad"
     }
    },
    "aee2363983ee4647a2aff35edb6f35d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_668bcd14eacf47638aecee5a3fe71330",
      "max": 90,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0826cf8dd39b493c9679db0cab2a9674",
      "value": 90
     }
    },
    "d37b98ed92e1493091ba99c3acea4660": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d84382de4ad64141b92859057bda4b4a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f32519912fa34ff99ff31367cae10338": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fc9d146a819f45bfb5614a36900ff63f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
