{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "72gErY_lDa7h",
    "outputId": "80e093f6-5371-4d11-dbe3-234b3077159c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: qwen-vl-utils in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (0.0.14)\n",
      "Requirement already satisfied: datasets in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (4.2.0)\n",
      "Requirement already satisfied: torch in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (2.9.0)\n",
      "Requirement already satisfied: torchvision in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (0.24.0)\n",
      "Requirement already satisfied: transformers in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (4.57.1)\n",
      "Requirement already satisfied: accelerate in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (1.11.0)\n",
      "Requirement already satisfied: av in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from qwen-vl-utils) (16.0.1)\n",
      "Requirement already satisfied: packaging in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from qwen-vl-utils) (25.0)\n",
      "Requirement already satisfied: pillow in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from qwen-vl-utils) (12.0.0)\n",
      "Requirement already satisfied: requests in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from qwen-vl-utils) (2.32.5)\n",
      "Requirement already satisfied: filelock in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from datasets) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from datasets) (2.3.4)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2025.9.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from datasets) (0.35.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.13.1)\n",
      "Requirement already satisfied: anyio in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: certifi in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.1.10)\n",
      "Requirement already satisfied: setuptools in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.0 in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from torch) (3.5.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from transformers) (2025.10.23)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: psutil in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from accelerate) (7.1.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from requests->qwen-vl-utils) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from requests->qwen-vl-utils) (2.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: ipywidgets in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (8.1.7)\n",
      "Requirement already satisfied: widgetsnbextension in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (4.0.14)\n",
      "Requirement already satisfied: comm>=0.1.3 in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from ipywidgets) (9.6.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from ipywidgets) (3.0.15)\n",
      "Requirement already satisfied: decorator in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
      "Requirement already satisfied: stack_data in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: wcwidth in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.14)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.5)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /home/jj/github/cs_sustainability/.venv/lib/python3.12/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages (works for both Colab and local Jupyter)\n",
    "!pip install -U qwen-vl-utils datasets torch torchvision transformers accelerate\n",
    "!pip install -U ipywidgets widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "r_vC1rxOCiVZ"
   },
   "outputs": [],
   "source": [
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# evaluation\n",
    "import os\n",
    "import re\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "1fYYLSAjZ85L"
   },
   "outputs": [],
   "source": [
    "## setting of important macros\n",
    "MODEL_ID = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
    "SUBSET_SIZE = 50\n",
    "SHUFFLE_BUFFER_SIZE = SUBSET_SIZE * 10\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122,
     "referenced_widgets": [
      "076c0d24260f4f18a43a9558ce664cb2",
      "2c22bae211bb4b3ab440c1ee987dd544",
      "aee2363983ee4647a2aff35edb6f35d1",
      "67a58360f1fe4789be2e9ab67695a3dc",
      "f32519912fa34ff99ff31367cae10338",
      "d84382de4ad64141b92859057bda4b4a",
      "7ffe2e595eea48ac81995d3c71c1af38",
      "668bcd14eacf47638aecee5a3fe71330",
      "0826cf8dd39b493c9679db0cab2a9674",
      "d37b98ed92e1493091ba99c3acea4660",
      "8910c01866dd4650a5bab066f517d96e"
     ]
    },
    "id": "QnQiZ8tpVWPw",
    "outputId": "8d59b49c-f381-4187-def3-2fcc85815460"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading VQAv2 dataset with streaming...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ef5ee8e11814db09553aa162679f4f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffle the dataset, buffer size of 500...\n",
      "Taking subset of 50 samples...\n",
      "Loaded 50 samples\n"
     ]
    }
   ],
   "source": [
    "# Load VQAv2 dataset - using the mirror version that doesn't require dataset scripts\n",
    "print(\"Loading VQAv2 dataset with streaming...\")\n",
    "dataset = load_dataset(\"Multimodal-Fatima/VQAv2_validation\", split=\"validation\", streaming=True)\n",
    "\n",
    "# Shuffle dataset before taking subset to ensure representative sampling\n",
    "print(f\"Shuffle the dataset, buffer size of {SHUFFLE_BUFFER_SIZE}...\")\n",
    "shuffled_dataset = dataset.shuffle(seed=SEED, buffer_size=SHUFFLE_BUFFER_SIZE)\n",
    "\n",
    "# Take subset and convert to list (needed for multiple iterations)\n",
    "print(f\"Taking subset of {SUBSET_SIZE} samples...\")\n",
    "samples = list(shuffled_dataset.take(SUBSET_SIZE))\n",
    "print(f\"Loaded {len(samples)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nw85-jT0XyLC",
    "outputId": "0913e13e-c906-44f0-f68d-02d1d3ed25a9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question_type': 'how many',\n",
       " 'multiple_choice_answer': '1',\n",
       " 'answers': ['1', '1', '1', '1', '1', '1', '1', '1', '1', '1'],\n",
       " 'answers_original': [{'answer': '1',\n",
       "   'answer_confidence': 'yes',\n",
       "   'answer_id': 1},\n",
       "  {'answer': '1', 'answer_confidence': 'yes', 'answer_id': 2},\n",
       "  {'answer': '1', 'answer_confidence': 'yes', 'answer_id': 3},\n",
       "  {'answer': '1', 'answer_confidence': 'yes', 'answer_id': 4},\n",
       "  {'answer': '1', 'answer_confidence': 'yes', 'answer_id': 5},\n",
       "  {'answer': '1', 'answer_confidence': 'yes', 'answer_id': 6},\n",
       "  {'answer': '1', 'answer_confidence': 'yes', 'answer_id': 7},\n",
       "  {'answer': '1', 'answer_confidence': 'yes', 'answer_id': 8},\n",
       "  {'answer': '1', 'answer_confidence': 'yes', 'answer_id': 9},\n",
       "  {'answer': '1', 'answer_confidence': 'yes', 'answer_id': 10}],\n",
       " 'id_image': 232309,\n",
       " 'answer_type': 'number',\n",
       " 'question_id': 232309002,\n",
       " 'question': 'How many kites are in the picture?',\n",
       " 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=500x332>,\n",
       " 'id': 169159,\n",
       " 'clip_tags_ViT_L_14': ['stunt kite',\n",
       "  'large kite flying',\n",
       "  'kiteportion',\n",
       "  '\"large kite',\n",
       "  'kite boarding'],\n",
       " 'blip_caption': 'a man flying a kite on the beach',\n",
       " 'LLM_Description_gpt3_downstream_tasks_visual_genome_ViT_L_14': ['kite-pulling',\n",
       "  'maneuvering the kite',\n",
       "  'string or cord connecting the kite to a person',\n",
       "  'kite-boarding',\n",
       "  'kite boarding'],\n",
       " 'DETA_detections_deta_swin_large_o365_coco_classes': [{'attribute': '',\n",
       "   'box': [120.47334289550781,\n",
       "    121.14210510253906,\n",
       "    226.51441955566406,\n",
       "    293.7930603027344],\n",
       "   'label': 'Person',\n",
       "   'location': 'middle',\n",
       "   'ratio': 0.11028972268104553,\n",
       "   'size': 'small',\n",
       "   'tag': ''},\n",
       "  {'attribute': '',\n",
       "   'box': [121.32807922363281,\n",
       "    270.19610595703125,\n",
       "    139.09259033203125,\n",
       "    293.84442138671875],\n",
       "   'label': 'Sneakers',\n",
       "   'location': 'lower left',\n",
       "   'ratio': 0.0025307275354862213,\n",
       "   'size': 'small',\n",
       "   'tag': ''},\n",
       "  {'attribute': '',\n",
       "   'box': [177.25653076171875,\n",
       "    284.4181213378906,\n",
       "    206.15231323242188,\n",
       "    290.9072570800781],\n",
       "   'label': 'Sneakers',\n",
       "   'location': 'lower middle',\n",
       "   'ratio': 0.001129570184275508,\n",
       "   'size': 'small',\n",
       "   'tag': ''},\n",
       "  {'attribute': '',\n",
       "   'box': [4.800379276275635,\n",
       "    37.91737747192383,\n",
       "    377.7641906738281,\n",
       "    296.6324157714844],\n",
       "   'label': 'Kite',\n",
       "   'location': 'middle',\n",
       "   'ratio': 0.5812731385231018,\n",
       "   'size': 'large',\n",
       "   'tag': ''},\n",
       "  {'attribute': '',\n",
       "   'box': [222.48748779296875,\n",
       "    36.33881759643555,\n",
       "    353.46234130859375,\n",
       "    205.1005401611328],\n",
       "   'label': 'Kite',\n",
       "   'location': 'middle',\n",
       "   'ratio': 0.13315385580062866,\n",
       "   'size': 'small',\n",
       "   'tag': ''}],\n",
       " 'DETA_detections_deta_swin_large_o365_clip_ViT_L_14': [{'attribute': 'string or cord connecting the kite to a person',\n",
       "   'box': [120.47334289550781,\n",
       "    121.14210510253906,\n",
       "    226.51441955566406,\n",
       "    293.7930603027344],\n",
       "   'label': 'Person',\n",
       "   'location': 'middle',\n",
       "   'ratio': 0.11028972268104553,\n",
       "   'size': 'small',\n",
       "   'tag': 'person flying kite'},\n",
       "  {'attribute': 'black markings on the feet and legs',\n",
       "   'box': [121.32807922363281,\n",
       "    270.19610595703125,\n",
       "    139.09259033203125,\n",
       "    293.84442138671875],\n",
       "   'label': 'Sneakers',\n",
       "   'location': 'lower left',\n",
       "   'ratio': 0.0025307275354862213,\n",
       "   'size': 'small',\n",
       "   'tag': 'black/brown shoe'},\n",
       "  {'attribute': 'kite-flying',\n",
       "   'box': [4.800379276275635,\n",
       "    37.91737747192383,\n",
       "    377.7641906738281,\n",
       "    296.6324157714844],\n",
       "   'label': 'Kite',\n",
       "   'location': 'middle',\n",
       "   'ratio': 0.5812731385231018,\n",
       "   'size': 'large',\n",
       "   'tag': 'large kite flying'},\n",
       "  {'attribute': 'dragon kite',\n",
       "   'box': [222.48748779296875,\n",
       "    36.33881759643555,\n",
       "    353.46234130859375,\n",
       "    205.1005401611328],\n",
       "   'label': 'Kite',\n",
       "   'location': 'middle',\n",
       "   'ratio': 0.13315385580062866,\n",
       "   'size': 'small',\n",
       "   'tag': 'dragon kite'}],\n",
       " 'DETA_detections_deta_swin_large_o365_clip_ViT_L_14_blip_caption': [{'attribute': 'string or cord connecting the kite to a person',\n",
       "   'box': [120.47334289550781,\n",
       "    121.14210510253906,\n",
       "    226.51441955566406,\n",
       "    293.7930603027344],\n",
       "   'caption': 'a man on the beach with a kite',\n",
       "   'label': 'Person',\n",
       "   'location': 'middle',\n",
       "   'ratio': 0.11028972268104553,\n",
       "   'size': 'small',\n",
       "   'tag': 'person flying kite'},\n",
       "  {'attribute': 'black markings on the feet and legs',\n",
       "   'box': [121.32807922363281,\n",
       "    270.19610595703125,\n",
       "    139.09259033203125,\n",
       "    293.84442138671875],\n",
       "   'caption': 'a black cat on the floor',\n",
       "   'label': 'Sneakers',\n",
       "   'location': 'lower left',\n",
       "   'ratio': 0.0025307275354862213,\n",
       "   'size': 'small',\n",
       "   'tag': 'black/brown shoe'},\n",
       "  {'attribute': 'kite-flying',\n",
       "   'box': [4.800379276275635,\n",
       "    37.91737747192383,\n",
       "    377.7641906738281,\n",
       "    296.6324157714844],\n",
       "   'caption': 'a man flying a kite on the beach',\n",
       "   'label': 'Kite',\n",
       "   'location': 'middle',\n",
       "   'ratio': 0.5812731385231018,\n",
       "   'size': 'large',\n",
       "   'tag': 'large kite flying'},\n",
       "  {'attribute': 'dragon kite',\n",
       "   'box': [222.48748779296875,\n",
       "    36.33881759643555,\n",
       "    353.46234130859375,\n",
       "    205.1005401611328],\n",
       "   'caption': 'a person flying a kite on the beach',\n",
       "   'label': 'Kite',\n",
       "   'location': 'middle',\n",
       "   'ratio': 0.13315385580062866,\n",
       "   'size': 'small',\n",
       "   'tag': 'dragon kite'}],\n",
       " 'Attributes_ViT_L_14_descriptors_text_davinci_003_full': ['kite which has brightly colored fabric or paper stretched over the frame',\n",
       "  'kite which is a light frame made of wood, plastic, or fiberglass',\n",
       "  'kite which is a tail made of fabric, string, or ribbon',\n",
       "  'kite (bird of prey) which is a medium-sized, graceful bird of prey',\n",
       "  'kite (bird of prey) which has hooked beak'],\n",
       " 'clip_tags_ViT_L_14_wo_openai': ['kite',\n",
       "  'kite',\n",
       "  'kite',\n",
       "  'parasail',\n",
       "  'glider'],\n",
       " 'clip_tags_ViT_L_14_with_openai': ['Kite sports',\n",
       "  'Sport kite',\n",
       "  'Kite landboarding',\n",
       "  'Kiteboating',\n",
       "  'Kite buggy'],\n",
       " 'clip_tags_LAION_ViT_H_14_2B_wo_openai': ['kite',\n",
       "  'kite',\n",
       "  'kite',\n",
       "  'windsock',\n",
       "  'windsock'],\n",
       " 'clip_tags_LAION_ViT_H_14_2B_with_openai': ['Kite sports',\n",
       "  'Kite skating',\n",
       "  'kite',\n",
       "  'Kite',\n",
       "  'kite'],\n",
       " 'clip_tags_LAION_ViT_bigG_14_2B_wo_openai': ['kite',\n",
       "  'kite',\n",
       "  'kite',\n",
       "  'windsock',\n",
       "  'windsock'],\n",
       " 'clip_tags_LAION_ViT_bigG_14_2B_with_openai': ['Kite skating',\n",
       "  'Sport kite',\n",
       "  'kite',\n",
       "  'kite',\n",
       "  'Kite'],\n",
       " 'Attributes_LAION_ViT_H_14_2B_descriptors_text_davinci_003_full': ['kite which is a light frame made of wood, plastic, or fiberglass',\n",
       "  'kite which is a tail made of fabric, string, or ribbon',\n",
       "  'kite which is a bridle attached to the frame that holds the kite in place',\n",
       "  'kite which has brightly colored fabric or paper stretched over the frame',\n",
       "  'kite which is a line to fly the kite attached to the bridle'],\n",
       " 'Attributes_LAION_ViT_bigG_14_2B_descriptors_text_davinci_003_full': ['kite which is a light frame made of wood, plastic, or fiberglass',\n",
       "  'kite which is a tail made of fabric, string, or ribbon',\n",
       "  'kite which is a line to fly the kite attached to the bridle',\n",
       "  'kite which has brightly colored fabric or paper stretched over the frame',\n",
       "  'kite which is a bridle attached to the frame that holds the kite in place'],\n",
       " 'DETA_detections_deta_swin_large_o365_coco_classes_caption_module_random': [{'attribute': '',\n",
       "   'box': [120.47334289550781,\n",
       "    121.14210510253906,\n",
       "    226.51441955566406,\n",
       "    293.7930603027344],\n",
       "   'captions_module': ['the man stands on the beach as he holds a kite',\n",
       "    'the guy is walking along the beach with his red and white kite',\n",
       "    'a man with a tie playing a game of kites on the beach',\n",
       "    'a person running on a beach with a kite',\n",
       "    'a male is being chased by an octopus shaped kite',\n",
       "    'a man on a beach flying a kite',\n",
       "    'man holding a kite next to a blue white and red kite',\n",
       "    'a man walking on beach walking with kite in hand',\n",
       "    'a man standing on a beach holding a kite',\n",
       "    'a man with a red kite is walking on sand',\n",
       "    'a man walking with an orange and black and red kite',\n",
       "    'a man flying a kite along a beach on a cloudy day',\n",
       "    'a man is walking near a beach holding a ribbon and kite',\n",
       "    'a man standing on top of a beach next to a kite',\n",
       "    'a man is on the beach with a kite',\n",
       "    'a man at the beach runs toward a red kite in his hand',\n",
       "    'a man is on a beach flying a kite',\n",
       "    'man flying a kite beside water near shore line',\n",
       "    'a man in shorts is flying a kite by the ocean',\n",
       "    'a man flying a kite on the beach',\n",
       "    'a man is walking in the sand with a kite',\n",
       "    'a man walking on a beach with a kite',\n",
       "    'a man standing on a beach flying a kite',\n",
       "    'a man on the beach flying a kite',\n",
       "    'a man running on a beach holding a kite',\n",
       "    'a man wearing a blue shirt flying a kite',\n",
       "    'an older man walking on the beach with a kite',\n",
       "    'a man who is walking with a kite in his hand on a beach',\n",
       "    'a man is standing on a beach flying a kite',\n",
       "    'a man running on the beach after getting his kite out of the water'],\n",
       "   'captions_module_filter': ['a man plays with streamers in a sea of water',\n",
       "    'adult walking by water with young boy on beach',\n",
       "    'this person is flying a kite on the beach',\n",
       "    'there is a man walking on a beach with a kite',\n",
       "    'a man is running along a wave breaking sandy beach',\n",
       "    'a man on a beach flies a kite on a beach',\n",
       "    'a person on the sand with a red kite',\n",
       "    'a man tries to get a kite from him',\n",
       "    'a person is going to fly a kite on the sandy beach',\n",
       "    'a man is on the beach flying a kite',\n",
       "    'a man with wet hair standing in the sand flying a red kite',\n",
       "    'a man walking on top of a beach next to the ocean',\n",
       "    'there is a man on the beach that is about to fly a kite',\n",
       "    'a man on a beach with a kite and waves behind him',\n",
       "    'a man flying a kite at the beach',\n",
       "    'a man walking on a sandy beach with a kite',\n",
       "    'a man flying a kite on a sand beach',\n",
       "    'a man is holding on to a string',\n",
       "    'this is an outdoor picture of a man at the beach',\n",
       "    'there is a male in a blue shirt flying a kite',\n",
       "    'a person walking by the water flying a kite',\n",
       "    'the man wears a blue shirt and the bright red tail band tie',\n",
       "    'a man on the beach with a kite in his hand',\n",
       "    'a man pulls a kite in both directions on the beach',\n",
       "    'a guy running on the beach with a kite',\n",
       "    'a person is walking on a beach while holding two ribbons with one end of the red blue and orange band',\n",
       "    'a man walking down a beach with a kite',\n",
       "    'a man on a beach with kite flying in the air',\n",
       "    'an image of a man standing on the beach flying a kite',\n",
       "    'a man is flying a blue kite on the beach'],\n",
       "   'label': 'Person',\n",
       "   'location': 'middle',\n",
       "   'ratio': 0.11028972268104553,\n",
       "   'size': 'small',\n",
       "   'tag': ''},\n",
       "  {'attribute': '',\n",
       "   'box': [121.32807922363281,\n",
       "    270.19610595703125,\n",
       "    139.09259033203125,\n",
       "    293.84442138671875],\n",
       "   'captions_module': ['a black shoe standing near a small cat',\n",
       "    'a person in jeans laying on a counter',\n",
       "    'a cat is sitting down on a stone shelf next to a persons feet',\n",
       "    'a man in a cap sitting in front of some books under a laptop',\n",
       "    'a dog laying down on a bathroom counter top',\n",
       "    'a man in black shoes is on the toilet',\n",
       "    'a close up of a white toilet with a person sitting next to it',\n",
       "    'a large black dog laying in its bed',\n",
       "    'a black cat sitting on top of foot pads',\n",
       "    'there is a dog laying on a blanket near a persons legs',\n",
       "    \"a person's hand holding a cat in their lap with a cat sitting on its legs\",\n",
       "    \"a close up of a person's shoe on the side of a road\",\n",
       "    'a gray cat stretching its head over a cupcake saucer on a sink',\n",
       "    'the person has their shoes off of the table',\n",
       "    'a black cat sitting on a kitchen counter next to a towel',\n",
       "    'a cat looks down, with a head resting on a sink top',\n",
       "    'a black and white photograph of a person on a computer',\n",
       "    'a black and white photo of a person walking',\n",
       "    'a black and white cat sleeping on a chair in the corner',\n",
       "    'a dog that is laying down on a floor',\n",
       "    'the foot of a person standing on an open gas stove',\n",
       "    'a black dog that is sitting in a chair',\n",
       "    'a black kitten sleeping on a floor with a cup and saucer in front of it',\n",
       "    'a white cat is lying next to a black laptop',\n",
       "    'a cat curled up by a window while sitting',\n",
       "    'a black cat sitting on top of a toilet brush',\n",
       "    'there is a pair of shoes with one one with both shoes open',\n",
       "    'a person using a pair of shoes to take a bath',\n",
       "    'black and white dog looking down at its left',\n",
       "    'a pair of black boots with white socks hanging down'],\n",
       "   'captions_module_filter': ['a cat on a chair in a bathroom',\n",
       "    'two cats resting on a chair on a black and white picture by',\n",
       "    'a black dog on a white and gray microwave oven',\n",
       "    'a black and white cat standing in an old, dirty urinal',\n",
       "    'a black dog, black boots sits in a chair',\n",
       "    'a black dog lays on a white counter',\n",
       "    'a large dog laying on a pair of shoes',\n",
       "    'a person sitting on a shoe with their shoe on',\n",
       "    'a black cat is sniffing a cat in the bathroom',\n",
       "    'there is a dog taking a nap in a bathtub',\n",
       "    'a black and white cat with its paw on a foot',\n",
       "    'a big black cat sitting on the floor in the bathroom',\n",
       "    'a picture of a cat lying on its owners foot',\n",
       "    'black and white cat reaching forward with paw on table',\n",
       "    'a picture of a cat that is sitting under a person',\n",
       "    'a dog is standing on the edge of a bathroom sink',\n",
       "    'a black cat standing at the seat of a black chair',\n",
       "    'a black cat stretching itself its paws out over a toilet',\n",
       "    'a cat sitting on top of a pair of heels',\n",
       "    'the feet of a person who is squatting down',\n",
       "    'a dog on a white towel and a toilet with a sink',\n",
       "    'sandals laying on a bench with the sole up',\n",
       "    \"man's foot and shoes taken from above\",\n",
       "    'a cat sitting on top of a black sneakers with a white kitten in front of a pair of socks',\n",
       "    'a dog peeking his head over a toilet seat',\n",
       "    'a cat is laying on the toilet seat and hiding',\n",
       "    'a cat sitting on top of a floor next to a person',\n",
       "    'a cat is peeking down at a pair of shoes',\n",
       "    'a black and white cats sitting on a tiled floor',\n",
       "    'a black cat standing on top of a kitchen counter'],\n",
       "   'label': 'Sneakers',\n",
       "   'location': 'lower left',\n",
       "   'ratio': 0.0025307275354862213,\n",
       "   'size': 'small',\n",
       "   'tag': ''},\n",
       "  {'attribute': '',\n",
       "   'box': [177.25653076171875,\n",
       "    284.4181213378906,\n",
       "    206.15231323242188,\n",
       "    290.9072570800781],\n",
       "   'captions_module': ['a living room with a chair and television is seen',\n",
       "    'the view of a tall television in a room',\n",
       "    'a table with multiple plates and some bananas',\n",
       "    'a blurry picture of three black chairs in a living room',\n",
       "    'a blurry photo of a living room and tv',\n",
       "    'two cups of smooth blueberries on a table',\n",
       "    'a picture of a kitchen scene with focus on the food',\n",
       "    'a little girl with her eyes closed in a kitchen with an opening curtain',\n",
       "    'a small toy is standing in a room with a window',\n",
       "    'the television is turned on and sitting on a stand',\n",
       "    'a room that has a television and a door',\n",
       "    'a blurry picture of a tv that is on',\n",
       "    'the small blue device is ready to be put on',\n",
       "    'a close up view of a clock on a table',\n",
       "    'the view of a man standing in a room from a blurred backdrop',\n",
       "    'a blurry image of a table with flowers and a large black clock',\n",
       "    'a white dog is running across a tiled hallway',\n",
       "    'a photo of a building from the street',\n",
       "    'a man standing in a hallway holding out his hand',\n",
       "    'a blurry picture of a television with a cat looking towards it',\n",
       "    'a tv with black curtain in a bathroom',\n",
       "    'this is an image of a close up of a speaker',\n",
       "    'looking onto a counter top that is also blurry',\n",
       "    'a close up of a mirror showing an abstract background',\n",
       "    'a piece of luggage with a clock in the background',\n",
       "    'there is a clock on the television set',\n",
       "    'a blurry photo of a small black cat',\n",
       "    'a large, round object in front of a blurry picture',\n",
       "    'a blurry photo of a television in a room',\n",
       "    'a large black microwave sitting next to a television'],\n",
       "   'captions_module_filter': ['an air cleaner sits in front of a mirror',\n",
       "    'view of a window with curtains with a blurry image in the background',\n",
       "    'very long exposure of someone standing in front of a window',\n",
       "    'a blurry picture of a tv screen in a living room',\n",
       "    'an blury photo of a tv set',\n",
       "    'a clock and window in a small room',\n",
       "    'black and white blurry photograph of a room with a clock',\n",
       "    'an odd blurry photograph of a tv and a television stand',\n",
       "    'a tv set sits on a wood table',\n",
       "    'person holding knife and knife holder, blurry with a window as background',\n",
       "    'blurry photograph of a plate of hotdogs and fries',\n",
       "    'a blurry image of a small dog next to a television set',\n",
       "    'the curtains that are closed in the room',\n",
       "    'there is a video game controller in the room',\n",
       "    'black sliding door seen from outside with blurred background',\n",
       "    'the reflection of a woman in black dresses in a room',\n",
       "    'a blurry picture of a television set in a living room',\n",
       "    'blurry shot of television with television in middle with chairs and a table in between it',\n",
       "    'a clock and television on wall near a large window',\n",
       "    'a luggage bag is next to the mirror',\n",
       "    'a close - up of black vases, and black glass',\n",
       "    'the window of a house is blurry and the tv monitor is turned on',\n",
       "    'a blurry picture of a door with a clock hanging on it',\n",
       "    'the television is on, against the wall as a blurry image',\n",
       "    'an indoor television set sitting next to a tree',\n",
       "    'a black cell phone in front of a door',\n",
       "    'a blurry photo of a chair and a mirror',\n",
       "    'there is a wine glass in the foreground',\n",
       "    'blurry view of a large living room with a television',\n",
       "    'blurry shot of bathroom showing toilet and sink'],\n",
       "   'label': 'Sneakers',\n",
       "   'location': 'lower middle',\n",
       "   'ratio': 0.001129570184275508,\n",
       "   'size': 'small',\n",
       "   'tag': ''},\n",
       "  {'attribute': '',\n",
       "   'box': [4.800379276275635,\n",
       "    37.91737747192383,\n",
       "    377.7641906738281,\n",
       "    296.6324157714844],\n",
       "   'captions_module': ['a person on a beach with a colorful kite',\n",
       "    'a man walking near water holding a kite',\n",
       "    'a man walks on the beach with a kite',\n",
       "    'a man with a kite at the beach with the ocean in the background',\n",
       "    'there is a man that is walking alone on the beach',\n",
       "    'a bird kite that is flying on the beach',\n",
       "    'a man is flying a kite by the beach',\n",
       "    'a man walks on the beach and flies a kite',\n",
       "    'a man holds onto a dragon tail kite along the beach',\n",
       "    'a man that is walking with a kite on a beach',\n",
       "    'a man on a beach flying a dragon kite',\n",
       "    'a man walks near a rainbow colored dragon kite',\n",
       "    'man flying a kite on the beach next to a body of water',\n",
       "    'a man in blue shirt holding a red long kite on the beach and walking to the water',\n",
       "    'a man is flying a kite on the beach',\n",
       "    'a man walking along the beach with a kite',\n",
       "    'a man trying to fly a kite at the beach',\n",
       "    'a man walks across a beach as a dragon kite approaches him',\n",
       "    'young man on beach holding kite on large beach',\n",
       "    'man walking on the beach with a kite',\n",
       "    'a man is walking along the beach as he flies a dragon kite',\n",
       "    'a person on a beach holding onto a colorful kite',\n",
       "    'a man holding onto a kite in a shape of a long kite',\n",
       "    'a man on a beach near the ocean making a dragon kite string tied to it and flying the kite',\n",
       "    'a man is walking with a kite on the beach',\n",
       "    'a man with a kite is on the beach',\n",
       "    'a man stands on the beach while flying a kite',\n",
       "    'a man is flying a kite near the water',\n",
       "    'a man walking on a beach with a kite',\n",
       "    'a colorful dragon kite is being flown on a beach'],\n",
       "   'captions_module_filter': ['a man preparing to launch a kite with a fish tail along the beach',\n",
       "    'man walking on the beach carrying a kite',\n",
       "    'the man is playing with the flying kite on the beach',\n",
       "    'a man standing on a beach in front of the ocean holding a kite string up for flying on the sand below the shore',\n",
       "    'this is a man flying a dragon kite on the beach',\n",
       "    'man walking on a beach holding a kite',\n",
       "    'a man is on the beach flying a kite',\n",
       "    'man flying and flying a colorful kite on the beach',\n",
       "    'man on the beach flying colorful kite',\n",
       "    'a man is flying a kite outside on the beach',\n",
       "    'man walking on the beach, holding a kite',\n",
       "    'a man holding a red dragon and flying a kite',\n",
       "    'the man is walking along the beach with his kite',\n",
       "    'man running a beach with a kite on his hand and the ocean beyond',\n",
       "    'a man is holding on to a kite flying near the water',\n",
       "    'a man is walking on the beach holding a kite which has a dragon tail',\n",
       "    'a man walking on the beach with a dragon head kite in hand',\n",
       "    'a man is pulling the string down the beach',\n",
       "    'a man walks with a kite on the wet sand',\n",
       "    'a man on a beach flying a kite',\n",
       "    'a man carrying a kite that has red string around it',\n",
       "    'a man on the beach running with a kite',\n",
       "    'a man walking down the beach with a kite',\n",
       "    'a guy pulling his kite along the beach',\n",
       "    'a man with a kite near a sea on a beach',\n",
       "    'a person walking along the beach with a dragon kite',\n",
       "    'the man is flying the kite on the beach',\n",
       "    'a man on a beach flying a kite next to the ocean',\n",
       "    'a man flying a dragon shaped kite at the beach',\n",
       "    'a man is standing next to the ocean with a kite on it'],\n",
       "   'label': 'Kite',\n",
       "   'location': 'middle',\n",
       "   'ratio': 0.5812731385231018,\n",
       "   'size': 'large',\n",
       "   'tag': ''},\n",
       "  {'attribute': '',\n",
       "   'box': [222.48748779296875,\n",
       "    36.33881759643555,\n",
       "    353.46234130859375,\n",
       "    205.1005401611328],\n",
       "   'captions_module': ['a large kite flying next to the ocean',\n",
       "    'a child and adult fly a blue and orange kite on a sand beach',\n",
       "    'a young girl running next to the sea on a beach on a nice winter day flying her dragon shaped kite',\n",
       "    'the kite in the shape of a dragon is being flown over the sand on the beach',\n",
       "    'a dragon like kite in flight next to the ocean',\n",
       "    'people by the ocean at shore with kites in the sand',\n",
       "    'one man is flying a kite on the beach',\n",
       "    'a dragon kite is flying in front of the water',\n",
       "    'a woman with a kite flying over a beach',\n",
       "    'a woman prepares to fly a large kite',\n",
       "    'a woman holding onto a colorful kite flying above the beach',\n",
       "    'a kid flying a big kite near the beach',\n",
       "    'a large kite flying in the sky over the ocean',\n",
       "    'the dragon kite flies low to the beach',\n",
       "    'man who is standing in the sand on a beach with a dragon kite',\n",
       "    'a man running down the beach with a kite',\n",
       "    'a woman walking with a kite on the beach',\n",
       "    'a young boy with a feather kite stands in the white surf of the beach beside the dark - haired dog',\n",
       "    'it looks kind of like it is playing near the ocean',\n",
       "    'two people on beach flying kites in the surf',\n",
       "    'a kite that appears to have wings painted out of red and blue sits on a beach, close to sea waves',\n",
       "    'a boy is flying his kite along the shoreline',\n",
       "    'a small child on the sand holding a kite',\n",
       "    'a young person carries a dragon kite down the beach',\n",
       "    'a boy in a wetsuit is flying his colorful kite',\n",
       "    'a woman is flying a dragon shaped kite',\n",
       "    'a girl with a dragon kite on the beach',\n",
       "    'a boy walking on the beach with a dragon kite',\n",
       "    'a small child flying a dragon kite on the beach in front of the ocean',\n",
       "    'a woman is running on the sand with a kite in the air'],\n",
       "   'captions_module_filter': ['two young people wearing dragon wings, flying',\n",
       "    'a child flying a dragon shaped kite on the beach',\n",
       "    'a kite is ready to fly, while a girl is barefoot in the sand',\n",
       "    'a child tries to fly a large dragon shaped kite',\n",
       "    'a person on the beach flying a kite',\n",
       "    'a kite that is being flown by the ocean',\n",
       "    'a picture of the young boy is flying a kite in the sand',\n",
       "    'a dog and a young man are playing with a kite',\n",
       "    'the big dragon kite is flying near the wave and the beach',\n",
       "    'a little kid is playing with a kite on the beach',\n",
       "    'a kid flying a kite with a red dragon tail',\n",
       "    'a dragon kite flies at the beach near the ocean',\n",
       "    'a young boy flying a dragon kite on the beach',\n",
       "    'a brown dog looking at a peacock kite by the ocean',\n",
       "    'a man flying a dragon kite on a beach',\n",
       "    'a small child running along the beach flying a dragon kite',\n",
       "    'a person on the beach flying a dragon kite',\n",
       "    'a child looking at a kite being flown by a person',\n",
       "    'colorful child at the beach getting ready to fly kite',\n",
       "    'a girl flying her kite on the beach',\n",
       "    'a colorful kite with wings on a beach',\n",
       "    'a boy flying a huge kite near the ocean',\n",
       "    'a person walks on a beach with a colorful bat kite',\n",
       "    'a child flying a kite on the beach with ocean waves',\n",
       "    'a person flying a orange dragon kite on the beach',\n",
       "    'a man with a kite standing on a beach',\n",
       "    'a kite that is flying at a beach',\n",
       "    'a person on the beach flying a colorful dragon kite',\n",
       "    'a dog walking on a beach next to a colorful bird kite',\n",
       "    'a young child with a dragon kite at a beach'],\n",
       "   'label': 'Kite',\n",
       "   'location': 'middle',\n",
       "   'ratio': 0.13315385580062866,\n",
       "   'size': 'small',\n",
       "   'tag': ''}]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview first sample\n",
    "samples[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "96f65b5039e74b789e042c8f086cf076",
      "1c02208ae3014312bdac3480bbaba754",
      "1e66d0c2d1924685b3b9849388976f16",
      "506335f15f2f466780b3243d7b31f5fd",
      "848eea9815814c2abdda91eb23c046ad",
      "035ad51cb6654f67a5516916b4953c7d",
      "73fa275c3b1e41f598393276c0a12a1f",
      "3a4c733821824ca1a442a8de1e363cb2",
      "545444cd30824736b7efdfc9590026f3",
      "fc9d146a819f45bfb5614a36900ff63f",
      "7a2e2d4116b64c698fd892fc92a697cd"
     ]
    },
    "id": "ckSCUHHtam_L",
    "outputId": "67119f98-e964-4d26-89ba-2f56a4d68438"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in local Jupyter notebook\n",
      "Device map: auto\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd70a67927cf4d9aada8a5f1fa4ff1f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n"
     ]
    }
   ],
   "source": [
    "# Check if running in Colab and set device accordingly\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    device_map = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(\"Running in Google Colab\")\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    device_map = \"auto\"\n",
    "    print(\"Running in local Jupyter notebook\")\n",
    "\n",
    "print(f\"Device map: {device_map}\")\n",
    "\n",
    "# Disable download progress bars to avoid widget errors\n",
    "# I guess it can be tested since it improves usability by a lot\n",
    "# try removing these lines and if a layout error appears, then put them back\n",
    "os.environ['HF_HUB_DISABLE_PROGRESS_BARS'] = '1'\n",
    "from huggingface_hub import utils as hf_utils\n",
    "hf_utils.disable_progress_bars()\n",
    "\n",
    "# Load the model on the available device(s)\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    dtype=\"auto\",\n",
    "    device_map=device_map\n",
    ")\n",
    "\n",
    "# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n",
    "# model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "#     \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     attn_implementation=\"flash_attention_2\",\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "# Re-enable progress bars for everything else (dataset loading, inference loops, etc.)\n",
    "del os.environ['HF_HUB_DISABLE_PROGRESS_BARS']\n",
    "hf_utils.enable_progress_bars()\n",
    "\n",
    "# default processor\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "## ONE OF THE EXPERIMENTS FOR VRAM USAGE ?\n",
    "# The default range for the number of visual tokens per image in the model is 4-16384. You can set min_pixels and max_pixels according to your needs, such as a token count range of 256-1280, to balance speed and memory usage.\n",
    "# min_pixels = 256*28*28\n",
    "# max_pixels = 1280*28*28\n",
    "#\n",
    "# processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JFtGMqYpCoSl",
    "outputId": "56174878-6b95-43de-b8aa-fa025858d716"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing VQAv2: 100%|██████████| 50/50 [00:19<00:00,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 50 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## ZERO-SHOT EVALUATION\n",
    "\n",
    "# Get the appropriate device (works for both Colab and local)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Process each sample\n",
    "results = []\n",
    "for idx, sample in enumerate(tqdm(samples, desc=\"Processing VQAv2\")):\n",
    "    # Prepare message for this sample\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": sample[\"image\"]},\n",
    "                {\"type\": \"text\", \"text\": sample[\"question\"]},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Preparation for inference\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "    # Inference: Generation of the output\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "    )\n",
    "\n",
    "    # Store results with all metadata needed for VQA evaluation\n",
    "    # FIXED: Extract answer strings from the list of answer dictionaries\n",
    "    # VQAv2 dataset format has answers as list of dicts: [{'answer': 'text'}, ...]\n",
    "    ground_truth_answers = [ans['answer'] if isinstance(ans, dict) else ans for ans in sample[\"answers\"]]\n",
    "    \n",
    "    results.append({\n",
    "        \"question\": sample[\"question\"],\n",
    "        \"predicted_answer\": output_text[0],\n",
    "        \"ground_truth_answers\": ground_truth_answers,  # Now contains list of strings\n",
    "        \"question_type\": sample.get(\"question_type\", \"unknown\"),\n",
    "        \"answer_type\": sample.get(\"answer_type\", \"unknown\"),\n",
    "        \"question_id\": sample.get(\"question_id\", idx)\n",
    "    })\n",
    "\n",
    "print(f\"\\nProcessed {len(results)} samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4NElCabBjvyN"
   },
   "source": [
    "34min to precess 850 images (accidental keyboard interrupt) ~ around 4-5min each 100 samples\n",
    "\n",
    "on my (jacopo) italian pc: around 0.5s per sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rKOiFCqWj6Rg",
    "outputId": "6d6c90c1-8da4-411c-f71f-86b7a4912f87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample result:\n",
      "{'question': 'How many kites are in the picture?', 'predicted_answer': 'There is one kite in the picture.', 'ground_truth_answers': ['1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], 'question_type': 'how many', 'answer_type': 'number', 'question_id': 232309002}\n",
      "\n",
      "Ground truth answers structure:\n",
      "['1', '1', '1', '1', '1', '1', '1', '1', '1', '1']\n"
     ]
    }
   ],
   "source": [
    "# Check the structure of results\n",
    "print(\"Sample result:\")\n",
    "print(results[0])\n",
    "print(\"\\nGround truth answers structure:\")\n",
    "print(results[0]['ground_truth_answers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'How many kites are in the picture?',\n",
       "  'predicted_answer': 'There is one kite in the picture.',\n",
       "  'ground_truth_answers': ['1', '1', '1', '1', '1', '1', '1', '1', '1', '1'],\n",
       "  'question_type': 'how many',\n",
       "  'answer_type': 'number',\n",
       "  'question_id': 232309002},\n",
       " {'question': 'What does the blender have on it?',\n",
       "  'predicted_answer': 'The blender has duct tape wrapped around it.',\n",
       "  'ground_truth_answers': ['fruit',\n",
       "   'fruit',\n",
       "   'tape',\n",
       "   'fruit',\n",
       "   'tape',\n",
       "   'buttons',\n",
       "   'duct tape',\n",
       "   'tape',\n",
       "   'duct tape',\n",
       "   'masking tape'],\n",
       "  'question_type': 'what does the',\n",
       "  'answer_type': 'other',\n",
       "  'question_id': 519359000},\n",
       " {'question': 'Is this player on an organized team?',\n",
       "  'predicted_answer': ' player on an organized team(641,191),(749,624)',\n",
       "  'ground_truth_answers': ['no',\n",
       "   'no',\n",
       "   'no',\n",
       "   'no',\n",
       "   'no',\n",
       "   'no',\n",
       "   'no',\n",
       "   'no',\n",
       "   'no',\n",
       "   'no'],\n",
       "  'question_type': 'is this',\n",
       "  'answer_type': 'yes/no',\n",
       "  'question_id': 232511000},\n",
       " {'question': 'What is the man have in his hand?',\n",
       "  'predicted_answer': 'The man is holding a maraca in his hand.',\n",
       "  'ground_truth_answers': ['bow',\n",
       "   'instrument',\n",
       "   'musical instrument',\n",
       "   'instrument',\n",
       "   'instrument',\n",
       "   'ukulele',\n",
       "   'musical instrument',\n",
       "   'instrument',\n",
       "   'instrument',\n",
       "   'string instrument'],\n",
       "  'question_type': 'what is the man',\n",
       "  'answer_type': 'other',\n",
       "  'question_id': 126123000},\n",
       " {'question': 'Is everyone looking at the skateboarder?',\n",
       "  'predicted_answer': ' everyone looking at the skateboarder(18,1),(993,355)',\n",
       "  'ground_truth_answers': ['no',\n",
       "   'yes',\n",
       "   'no',\n",
       "   'no',\n",
       "   'no',\n",
       "   'no',\n",
       "   'no',\n",
       "   'no',\n",
       "   'yes',\n",
       "   'no'],\n",
       "  'question_type': 'is',\n",
       "  'answer_type': 'yes/no',\n",
       "  'question_id': 363522000},\n",
       " {'question': 'What are they eating?',\n",
       "  'predicted_answer': 'I am not sure what they are eating. It could be breakfast, lunch, or dinner.',\n",
       "  'ground_truth_answers': ['breakfast',\n",
       "   'bagels, omelets, pancakes, fruit',\n",
       "   'breakfast',\n",
       "   'breakfast',\n",
       "   'breakfast',\n",
       "   'breakfast',\n",
       "   'breakfast',\n",
       "   'breakfast',\n",
       "   'food',\n",
       "   'breakfast'],\n",
       "  'question_type': 'what are',\n",
       "  'answer_type': 'other',\n",
       "  'question_id': 363666003},\n",
       " {'question': 'Is the kite a dragon?',\n",
       "  'predicted_answer': ' the kite is a dragon(495,105),(709,300)',\n",
       "  'ground_truth_answers': ['yes',\n",
       "   'yes',\n",
       "   'yes',\n",
       "   'yes',\n",
       "   'yes',\n",
       "   'yes',\n",
       "   'yes',\n",
       "   'yes',\n",
       "   'no',\n",
       "   'yes'],\n",
       "  'question_type': 'is the',\n",
       "  'answer_type': 'yes/no',\n",
       "  'question_id': 232309000},\n",
       " {'question': 'Is there a smoke stack?',\n",
       "  'predicted_answer': 'Yes, there is a smoke stack in the image.',\n",
       "  'ground_truth_answers': ['yes',\n",
       "   'yes',\n",
       "   'yes',\n",
       "   'yes',\n",
       "   'yes',\n",
       "   'yes',\n",
       "   'yes',\n",
       "   'yes',\n",
       "   'yes',\n",
       "   'yes'],\n",
       "  'question_type': 'is there a',\n",
       "  'answer_type': 'yes/no',\n",
       "  'question_id': 232538000},\n",
       " {'question': 'What brand is this toaster?',\n",
       "  'predicted_answer': 'I am not sure what brand this toaster is. It could be Oster, Cuisinart, or Hamilton Beach.',\n",
       "  'ground_truth_answers': [\"can't tell\",\n",
       "   'sony',\n",
       "   'oster',\n",
       "   'oster',\n",
       "   'osten',\n",
       "   'oster',\n",
       "   'oster',\n",
       "   'oster',\n",
       "   'ohen',\n",
       "   'oster'],\n",
       "  'question_type': 'what brand',\n",
       "  'answer_type': 'other',\n",
       "  'question_id': 232348001},\n",
       " {'question': 'What are the men holding in their hands?',\n",
       "  'predicted_answer': 'The men are holding ski poles in their hands.',\n",
       "  'ground_truth_answers': ['ski poles',\n",
       "   'poles',\n",
       "   'ski poles',\n",
       "   'ski poles',\n",
       "   'ski poles',\n",
       "   'ski poles',\n",
       "   'ski poles',\n",
       "   'ski sticks',\n",
       "   'ski poles',\n",
       "   'ski poles'],\n",
       "  'question_type': 'what are the',\n",
       "  'answer_type': 'other',\n",
       "  'question_id': 494456002}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VQAv2 Official Evaluation\n",
    "\n",
    "Now we'll use the official VQA evaluation toolkit to compute accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install official VQA evaluation toolkit\n",
    "!pip install --quiet matplotlib scikit-image\n",
    "!git clone https://github.com/GT-Vision-Lab/VQA.git\n",
    "!wget -q https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Annotations_Val_mscoco.zip\n",
    "!wget -q https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Questions_Val_mscoco.zip\n",
    "!unzip -q v2_Annotations_Val_mscoco.zip\n",
    "!unzip -q v2_Questions_Val_mscoco.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "\n",
    "# Add VQA toolkit to path\n",
    "sys.path.append('VQA/PythonEvaluationTools')\n",
    "sys.path.append('VQA/PythonHelperTools')\n",
    "\n",
    "# Format results for VQA evaluation\n",
    "# The official toolkit expects a list of dicts with 'question_id' and 'answer'\n",
    "vqa_results = [\n",
    "    {\n",
    "        'question_id': int(result['question_id']),\n",
    "        'answer': result['predicted_answer']  # Raw model output\n",
    "    }\n",
    "    for result in results\n",
    "]\n",
    "\n",
    "# Save results in VQA format\n",
    "with open('qwen2b_vqa_results.json', 'w') as f:\n",
    "    json.dump(vqa_results, f)\n",
    "\n",
    "print(f\"Saved {len(vqa_results)} results to qwen2b_vqa_results.json\")\n",
    "print(f\"\\nExample result format:\")\n",
    "print(json.dumps(vqa_results[0], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vqa import VQA\n",
    "from vqaEval import VQAEval\n",
    "\n",
    "# Paths to VQAv2 annotation files\n",
    "annFile = 'v2_mscoco_val2014_annotations.json'\n",
    "quesFile = 'v2_OpenEnded_mscoco_val2014_questions.json'\n",
    "resFile = 'qwen2b_vqa_results.json'\n",
    "\n",
    "# Load VQA annotations and questions\n",
    "print(\"Loading VQA annotations and questions...\")\n",
    "vqa = VQA(annFile, quesFile)\n",
    "\n",
    "# Load results\n",
    "print(\"Loading results...\")\n",
    "vqaRes = vqa.loadRes(resFile, quesFile)\n",
    "\n",
    "# Create VQAEval object and evaluate\n",
    "print(\"\\nEvaluating results...\")\n",
    "vqaEval = VQAEval(vqa, vqaRes, n=2)  # n=2 for VQAv2\n",
    "vqaEval.evaluate()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"VQAv2 EVALUATION RESULTS\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall accuracy\n",
    "print(f\"\\n{'Overall Accuracy:':<30} {vqaEval.accuracy['overall']:.2f}%\")\n",
    "\n",
    "# Accuracy by answer type\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ACCURACY BY ANSWER TYPE\")\n",
    "print(\"=\"*50)\n",
    "for ansType in vqaEval.accuracy['perAnswerType']:\n",
    "    print(f\"{ansType:<30} {vqaEval.accuracy['perAnswerType'][ansType]:.2f}%\")\n",
    "\n",
    "# Accuracy by question type (top 10)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ACCURACY BY QUESTION TYPE (Top 10)\")\n",
    "print(\"=\"*50)\n",
    "quesTypes = sorted(vqaEval.accuracy['perQuestionType'].items(), \n",
    "                   key=lambda x: x[1], reverse=True)[:10]\n",
    "for quesType, acc in quesTypes:\n",
    "    print(f\"{quesType:<30} {acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some example predictions vs ground truth\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SAMPLE PREDICTIONS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "sample_indices = random.sample(range(len(results)), min(5, len(results)))\n",
    "\n",
    "for idx in sample_indices:\n",
    "    result = results[idx]\n",
    "    print(f\"\\nQuestion: {result['question']}\")\n",
    "    print(f\"Predicted: {result['predicted_answer']}\")\n",
    "    print(f\"Ground Truth: {result['ground_truth_answers'][:3]}...\")  # Show first 3\n",
    "    print(f\"Type: {result['answer_type']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation summary\n",
    "eval_summary = {\n",
    "    'model': MODEL_ID,\n",
    "    'subset_size': len(results),\n",
    "    'overall_accuracy': vqaEval.accuracy['overall'],\n",
    "    'accuracy_by_answer_type': vqaEval.accuracy['perAnswerType'],\n",
    "    'accuracy_by_question_type': vqaEval.accuracy['perQuestionType']\n",
    "}\n",
    "\n",
    "with open('qwen2b_evaluation_summary.json', 'w') as f:\n",
    "    json.dump(eval_summary, f, indent=2)\n",
    "\n",
    "print(\"\\nEvaluation summary saved to qwen2b_evaluation_summary.json\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "035ad51cb6654f67a5516916b4953c7d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "076c0d24260f4f18a43a9558ce664cb2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2c22bae211bb4b3ab440c1ee987dd544",
       "IPY_MODEL_aee2363983ee4647a2aff35edb6f35d1",
       "IPY_MODEL_67a58360f1fe4789be2e9ab67695a3dc"
      ],
      "layout": "IPY_MODEL_f32519912fa34ff99ff31367cae10338"
     }
    },
    "0826cf8dd39b493c9679db0cab2a9674": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1c02208ae3014312bdac3480bbaba754": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_035ad51cb6654f67a5516916b4953c7d",
      "placeholder": "​",
      "style": "IPY_MODEL_73fa275c3b1e41f598393276c0a12a1f",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "1e66d0c2d1924685b3b9849388976f16": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3a4c733821824ca1a442a8de1e363cb2",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_545444cd30824736b7efdfc9590026f3",
      "value": 2
     }
    },
    "2c22bae211bb4b3ab440c1ee987dd544": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d84382de4ad64141b92859057bda4b4a",
      "placeholder": "​",
      "style": "IPY_MODEL_7ffe2e595eea48ac81995d3c71c1af38",
      "value": "Resolving data files: 100%"
     }
    },
    "3a4c733821824ca1a442a8de1e363cb2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "506335f15f2f466780b3243d7b31f5fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fc9d146a819f45bfb5614a36900ff63f",
      "placeholder": "​",
      "style": "IPY_MODEL_7a2e2d4116b64c698fd892fc92a697cd",
      "value": " 2/2 [00:22&lt;00:00,  9.49s/it]"
     }
    },
    "545444cd30824736b7efdfc9590026f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "668bcd14eacf47638aecee5a3fe71330": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "67a58360f1fe4789be2e9ab67695a3dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d37b98ed92e1493091ba99c3acea4660",
      "placeholder": "​",
      "style": "IPY_MODEL_8910c01866dd4650a5bab066f517d96e",
      "value": " 90/90 [00:00&lt;00:00, 1610.22it/s]"
     }
    },
    "73fa275c3b1e41f598393276c0a12a1f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7a2e2d4116b64c698fd892fc92a697cd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7ffe2e595eea48ac81995d3c71c1af38": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "848eea9815814c2abdda91eb23c046ad": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8910c01866dd4650a5bab066f517d96e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "96f65b5039e74b789e042c8f086cf076": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1c02208ae3014312bdac3480bbaba754",
       "IPY_MODEL_1e66d0c2d1924685b3b9849388976f16",
       "IPY_MODEL_506335f15f2f466780b3243d7b31f5fd"
      ],
      "layout": "IPY_MODEL_848eea9815814c2abdda91eb23c046ad"
     }
    },
    "aee2363983ee4647a2aff35edb6f35d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_668bcd14eacf47638aecee5a3fe71330",
      "max": 90,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0826cf8dd39b493c9679db0cab2a9674",
      "value": 90
     }
    },
    "d37b98ed92e1493091ba99c3acea4660": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d84382de4ad64141b92859057bda4b4a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f32519912fa34ff99ff31367cae10338": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fc9d146a819f45bfb5614a36900ff63f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
