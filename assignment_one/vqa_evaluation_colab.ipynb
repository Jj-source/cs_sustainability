{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title_cell"
   },
   "source": [
    "# VQA Evaluation for Verbose Answers - Adapted for Colab\n",
    "\n",
    "This notebook adapts the official VQA evaluation code from https://github.com/GT-Vision-Lab/VQA to work in Google Colab for evaluating verbose answers from VQAv2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_cell"
   },
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_dependencies"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install qwen-vl-utils datasets torch torchvision transformers accelerate\n",
    "!pip install ipywidgets widgetsnbextension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download_cell"
   },
   "source": [
    "## 2. Download VQA Evaluation Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_vqa_code"
   },
   "outputs": [],
   "source": [
    "# Download VQA evaluation files from GitHub\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('vqa_eval', exist_ok=True)\n",
    "\n",
    "# Base URL for raw GitHub files\n",
    "base_url = 'https://raw.githubusercontent.com/GT-Vision-Lab/VQA/master/PythonEvaluationTools/'\n",
    "\n",
    "# Download vqaEval.py\n",
    "print('Downloading vqaEval.py...')\n",
    "vqa_eval_url = base_url + 'vqaEvaluation/vqaEval.py'\n",
    "urllib.request.urlretrieve(vqa_eval_url, 'vqa_eval/vqaEval.py')\n",
    "\n",
    "# Download vqa.py (helper tools)\n",
    "print('Downloading vqa.py...')\n",
    "vqa_url = 'https://raw.githubusercontent.com/GT-Vision-Lab/VQA/master/PythonHelperTools/vqaTools/vqa.py'\n",
    "urllib.request.urlretrieve(vqa_url, 'vqa_eval/vqa.py')\n",
    "\n",
    "print('Download complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fix_python3"
   },
   "source": [
    "## 3. Fix Python 2 to Python 3 Compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "convert_to_python3"
   },
   "outputs": [],
   "source": [
    "# Convert Python 2 print statements to Python 3\n",
    "import re\n",
    "\n",
    "def convert_py2_to_py3(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Replace print statements\n",
    "    content = re.sub(r\"print '([^']*)'\", r\"print('\\1')\", content)\n",
    "    content = re.sub(r'print \"([^\"]*)\"', r'print(\"\\1\")', content)\n",
    "    content = re.sub(r\"print '%s: %s'%\\(([^,]+), ([^)]+)\\)\", r\"print(f'{\\1}: {\\2}')\", content)\n",
    "    content = re.sub(r\"print '([^']*)'[ ]*%[ ]*\\(([^)]*)\\)\", r\"print('\\1'.format(\\2))\", content)\n",
    "    content = re.sub(r'print \"([^\"]*)\"[ ]*%[ ]*\\(([^)]*)\\)', r'print(\"\\1\".format(\\2))', content)\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(content)\n",
    "    print(f'Converted {filename} to Python 3')\n",
    "\n",
    "convert_py2_to_py3('vqa_eval/vqaEval.py')\n",
    "convert_py2_to_py3('vqa_eval/vqa.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vqa_classes"
   },
   "source": [
    "## 4. VQA Evaluation Classes (Python 3 Compatible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vqa_eval_class"
   },
   "outputs": [],
   "source": [
    "# VQA Evaluation class adapted for Python 3 and verbose answers\n",
    "import re\n",
    "import json\n",
    "\n",
    "class VQAEval:\n",
    "    def __init__(self, vqa, vqaRes, n=2):\n",
    "        self.n = n\n",
    "        self.accuracy = {}\n",
    "        self.evalQA = {}\n",
    "        self.evalQuesType = {}\n",
    "        self.evalAnsType = {}\n",
    "        self.vqa = vqa\n",
    "        self.vqaRes = vqaRes\n",
    "        self.params = {'question_id': vqa.getQuesIds()}\n",
    "        self.contractions = {\n",
    "            \"aint\": \"ain't\", \"arent\": \"aren't\", \"cant\": \"can't\", \"couldve\": \"could've\",\n",
    "            \"couldnt\": \"couldn't\", \"couldn'tve\": \"couldn't've\", \"couldnt've\": \"couldn't've\",\n",
    "            \"didnt\": \"didn't\", \"doesnt\": \"doesn't\", \"dont\": \"don't\", \"hadnt\": \"hadn't\",\n",
    "            \"hadnt've\": \"hadn't've\", \"hadn'tve\": \"hadn't've\", \"hasnt\": \"hasn't\",\n",
    "            \"havent\": \"haven't\", \"hed\": \"he'd\", \"hed've\": \"he'd've\", \"he'dve\": \"he'd've\",\n",
    "            \"hes\": \"he's\", \"howd\": \"how'd\", \"howll\": \"how'll\", \"hows\": \"how's\",\n",
    "            \"Id've\": \"I'd've\", \"I'dve\": \"I'd've\", \"Im\": \"I'm\", \"Ive\": \"I've\",\n",
    "            \"isnt\": \"isn't\", \"itd\": \"it'd\", \"itd've\": \"it'd've\", \"it'dve\": \"it'd've\",\n",
    "            \"itll\": \"it'll\", \"let's\": \"let's\", \"maam\": \"ma'am\", \"mightnt\": \"mightn't\",\n",
    "            \"mightnt've\": \"mightn't've\", \"mightn'tve\": \"mightn't've\", \"mightve\": \"might've\",\n",
    "            \"mustnt\": \"mustn't\", \"mustve\": \"must've\", \"neednt\": \"needn't\",\n",
    "            \"notve\": \"not've\", \"oclock\": \"o'clock\", \"oughtnt\": \"oughtn't\",\n",
    "            \"ow's'at\": \"'ow's'at\", \"'ows'at\": \"'ow's'at\", \"'ow'sat\": \"'ow's'at\",\n",
    "            \"shant\": \"shan't\", \"shed've\": \"she'd've\", \"she'dve\": \"she'd've\",\n",
    "            \"shes\": \"she's\", \"shouldve\": \"should've\", \"shouldnt\": \"shouldn't\",\n",
    "            \"shouldnt've\": \"shouldn't've\", \"shouldn'tve\": \"shouldn't've\",\n",
    "            \"somebody'd\": \"somebodyd\", \"somebodyd've\": \"somebody'd've\",\n",
    "            \"somebody'dve\": \"somebody'd've\", \"somebodyll\": \"somebody'll\",\n",
    "            \"somebodys\": \"somebody's\", \"someoned\": \"someone'd\",\n",
    "            \"someoned've\": \"someone'd've\", \"someone'dve\": \"someone'd've\",\n",
    "            \"someonell\": \"someone'll\", \"someones\": \"someone's\", \"somethingd\": \"something'd\",\n",
    "            \"somethingd've\": \"something'd've\", \"something'dve\": \"something'd've\",\n",
    "            \"somethingll\": \"something'll\", \"thats\": \"that's\", \"thered\": \"there'd\",\n",
    "            \"thered've\": \"there'd've\", \"there'dve\": \"there'd've\", \"therere\": \"there're\",\n",
    "            \"theres\": \"there's\", \"theyd\": \"they'd\", \"theyd've\": \"they'd've\",\n",
    "            \"they'dve\": \"they'd've\", \"theyll\": \"they'll\", \"theyre\": \"they're\",\n",
    "            \"theyve\": \"they've\", \"twas\": \"'twas\", \"wasnt\": \"wasn't\",\n",
    "            \"wed've\": \"we'd've\", \"we'dve\": \"we'd've\", \"weve\": \"we've\",\n",
    "            \"werent\": \"weren't\", \"whatll\": \"what'll\", \"whatre\": \"what're\",\n",
    "            \"whats\": \"what's\", \"whatve\": \"what've\", \"whens\": \"when's\",\n",
    "            \"whered\": \"where'd\", \"wheres\": \"where's\", \"whereve\": \"where've\",\n",
    "            \"whod\": \"who'd\", \"whod've\": \"who'd've\", \"who'dve\": \"who'd've\",\n",
    "            \"wholl\": \"who'll\", \"whos\": \"who's\", \"whove\": \"who've\", \"whyll\": \"why'll\",\n",
    "            \"whyre\": \"why're\", \"whys\": \"why's\", \"wont\": \"won't\", \"wouldve\": \"would've\",\n",
    "            \"wouldnt\": \"wouldn't\", \"wouldnt've\": \"wouldn't've\", \"wouldn'tve\": \"wouldn't've\",\n",
    "            \"yall\": \"y'all\", \"yall'll\": \"y'all'll\", \"y'allll\": \"y'all'll\",\n",
    "            \"yall'd've\": \"y'all'd've\", \"y'alld've\": \"y'all'd've\", \"y'all'dve\": \"y'all'd've\",\n",
    "            \"youd\": \"you'd\", \"youd've\": \"you'd've\", \"you'dve\": \"you'd've\",\n",
    "            \"youll\": \"you'll\", \"youre\": \"you're\", \"youve\": \"you've\"\n",
    "        }\n",
    "        self.manualMap = {\n",
    "            'none': '0',\n",
    "            'zero': '0',\n",
    "            'one': '1',\n",
    "            'two': '2',\n",
    "            'three': '3',\n",
    "            'four': '4',\n",
    "            'five': '5',\n",
    "            'six': '6',\n",
    "            'seven': '7',\n",
    "            'eight': '8',\n",
    "            'nine': '9',\n",
    "            'ten': '10'\n",
    "        }\n",
    "        self.articles = ['a', 'an', 'the']\n",
    "        self.periodStrip = re.compile(\"(?!<=\\d)(\\.)(?!\\d)\")\n",
    "        self.commaStrip = re.compile(\"(\\d)(\\,)(\\d)\")\n",
    "        self.punct = [';', r\"/\", '[', ']', '\"', '{', '}',\n",
    "                      '(', ')', '=', '+', '\\\\', '_', '-',\n",
    "                      '>', '<', '@', '`', ',', '?', '!']\n",
    "\n",
    "    def evaluate(self, quesIds=None):\n",
    "        if quesIds == None:\n",
    "            quesIds = [quesId for quesId in self.params['question_id']]\n",
    "        gts = {}\n",
    "        res = {}\n",
    "        for quesId in quesIds:\n",
    "            gts[quesId] = self.vqa.qa[quesId]\n",
    "            res[quesId] = self.vqaRes.qa[quesId]\n",
    "\n",
    "        # Compute accuracy\n",
    "        accQA = []\n",
    "        accQuesType = {}\n",
    "        accAnsType = {}\n",
    "        print(\"Computing accuracy\")\n",
    "        \n",
    "        for quesId in quesIds:\n",
    "            resAns = res[quesId]['answer']\n",
    "            resAns = resAns.replace('\\n', ' ')\n",
    "            resAns = resAns.replace('\\t', ' ')\n",
    "            resAns = resAns.strip()\n",
    "            \n",
    "            gtAcc = []\n",
    "            gtAnswers = [ans['answer'] for ans in gts[quesId]['answers']]\n",
    "            \n",
    "            if len(set(gtAnswers)) > 1:\n",
    "                for ansDic in gts[quesId]['answers']:\n",
    "                    ansDic['answer'] = self.processPunctuation(ansDic['answer'])\n",
    "                    ansDic['answer'] = self.processDigitArticle(ansDic['answer'])\n",
    "                resAns = self.processPunctuation(resAns)\n",
    "                resAns = self.processDigitArticle(resAns)\n",
    "\n",
    "            for gtAnsDatum in gts[quesId]['answers']:\n",
    "                otherGTAns = [item for item in gts[quesId]['answers'] if item != gtAnsDatum]\n",
    "                matchingAns = [item for item in otherGTAns if item['answer'] == resAns]\n",
    "                acc = min(1, float(len(matchingAns)) / 3)\n",
    "                gtAcc.append(acc)\n",
    "            \n",
    "            quesType = gts[quesId]['question_type']\n",
    "            ansType = gts[quesId]['answer_type']\n",
    "            avgGTAcc = float(sum(gtAcc)) / len(gtAcc)\n",
    "            accQA.append(avgGTAcc)\n",
    "            \n",
    "            if quesType not in accQuesType:\n",
    "                accQuesType[quesType] = []\n",
    "            accQuesType[quesType].append(avgGTAcc)\n",
    "            \n",
    "            if ansType not in accAnsType:\n",
    "                accAnsType[ansType] = []\n",
    "            accAnsType[ansType].append(avgGTAcc)\n",
    "            \n",
    "            self.setEvalQA(quesId, avgGTAcc)\n",
    "            self.setEvalQuesType(quesId, quesType, avgGTAcc)\n",
    "            self.setEvalAnsType(quesId, ansType, avgGTAcc)\n",
    "\n",
    "        self.setAccuracy(accQA, accQuesType, accAnsType)\n",
    "        print(\"Done computing accuracy\")\n",
    "\n",
    "    def processPunctuation(self, inText):\n",
    "        outText = inText\n",
    "        for p in self.punct:\n",
    "            if (p + ' ' in inText or ' ' + p in inText) or (re.search(self.commaStrip, inText) != None):\n",
    "                outText = outText.replace(p, '')\n",
    "            else:\n",
    "                outText = outText.replace(p, ' ')\n",
    "        outText = self.periodStrip.sub(\"\", outText, re.UNICODE)\n",
    "        return outText\n",
    "\n",
    "    def processDigitArticle(self, inText):\n",
    "        outText = []\n",
    "        tempText = inText.lower().split()\n",
    "        for word in tempText:\n",
    "            word = self.manualMap.setdefault(word, word)\n",
    "            if word not in self.articles:\n",
    "                outText.append(word)\n",
    "            else:\n",
    "                pass\n",
    "        for wordId, word in enumerate(outText):\n",
    "            if word in self.contractions:\n",
    "                outText[wordId] = self.contractions[word]\n",
    "        outText = ' '.join(outText)\n",
    "        return outText\n",
    "\n",
    "    def setAccuracy(self, accQA, accQuesType, accAnsType):\n",
    "        self.accuracy['overall'] = round(100 * float(sum(accQA)) / len(accQA), self.n)\n",
    "        self.accuracy['perQuestionType'] = {\n",
    "            quesType: round(100 * float(sum(accQuesType[quesType])) / len(accQuesType[quesType]), self.n)\n",
    "            for quesType in accQuesType\n",
    "        }\n",
    "        self.accuracy['perAnswerType'] = {\n",
    "            ansType: round(100 * float(sum(accAnsType[ansType])) / len(accAnsType[ansType]), self.n)\n",
    "            for ansType in accAnsType\n",
    "        }\n",
    "\n",
    "    def setEvalQA(self, quesId, acc):\n",
    "        self.evalQA[quesId] = round(100 * acc, self.n)\n",
    "\n",
    "    def setEvalQuesType(self, quesId, quesType, acc):\n",
    "        if quesType not in self.evalQuesType:\n",
    "            self.evalQuesType[quesType] = {}\n",
    "        self.evalQuesType[quesType][quesId] = round(100 * acc, self.n)\n",
    "\n",
    "    def setEvalAnsType(self, quesId, ansType, acc):\n",
    "        if ansType not in self.evalAnsType:\n",
    "            self.evalAnsType[ansType] = {}\n",
    "        self.evalAnsType[ansType][quesId] = round(100 * acc, self.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vqa_class"
   },
   "outputs": [],
   "source": [
    "# VQA class for loading annotations and questions\n",
    "import json\n",
    "import datetime\n",
    "import copy\n",
    "\n",
    "class VQA:\n",
    "    def __init__(self, annotation_file=None, question_file=None):\n",
    "        \"\"\"\n",
    "        Constructor of VQA helper class for reading and visualizing questions and answers.\n",
    "        :param annotation_file (str): location of VQA annotation file\n",
    "        :param question_file (str): location of VQA question file\n",
    "        \"\"\"\n",
    "        # load dataset\n",
    "        self.dataset = {}\n",
    "        self.questions = {}\n",
    "        self.qa = {}\n",
    "        self.qqa = {}\n",
    "        self.imgToQA = {}\n",
    "        \n",
    "        if annotation_file is not None and question_file is not None:\n",
    "            print('Loading VQA annotations and questions into memory...')\n",
    "            time_t = datetime.datetime.utcnow()\n",
    "            dataset = json.load(open(annotation_file, 'r'))\n",
    "            questions = json.load(open(question_file, 'r'))\n",
    "            print(datetime.datetime.utcnow() - time_t)\n",
    "            self.dataset = dataset\n",
    "            self.questions = questions\n",
    "            self.createIndex()\n",
    "\n",
    "    def createIndex(self):\n",
    "        # create index\n",
    "        print('Creating index...')\n",
    "        imgToQA = {ann['image_id']: [] for ann in self.dataset['annotations']}\n",
    "        qa = {ann['question_id']: [] for ann in self.dataset['annotations']}\n",
    "        qqa = {ann['question_id']: [] for ann in self.dataset['annotations']}\n",
    "        for ann in self.dataset['annotations']:\n",
    "            imgToQA[ann['image_id']] += [ann]\n",
    "            qa[ann['question_id']] = ann\n",
    "        for ques in self.questions['questions']:\n",
    "            qqa[ques['question_id']] = ques\n",
    "        print('Index created!')\n",
    "\n",
    "        # create class members\n",
    "        self.qa = qa\n",
    "        self.qqa = qqa\n",
    "        self.imgToQA = imgToQA\n",
    "\n",
    "    def info(self):\n",
    "        \"\"\"\n",
    "        Print information about the VQA annotation file.\n",
    "        \"\"\"\n",
    "        for key, value in self.dataset['info'].items():\n",
    "            print(f'{key}: {value}')\n",
    "\n",
    "    def getQuesIds(self, imgIds=[], quesTypes=[], ansTypes=[]):\n",
    "        \"\"\"\n",
    "        Get question ids that satisfy given filter conditions.\n",
    "        \"\"\"\n",
    "        imgIds = imgIds if type(imgIds) == list else [imgIds]\n",
    "        quesTypes = quesTypes if type(quesTypes) == list else [quesTypes]\n",
    "        ansTypes = ansTypes if type(ansTypes) == list else [ansTypes]\n",
    "\n",
    "        if len(imgIds) == len(quesTypes) == len(ansTypes) == 0:\n",
    "            anns = self.dataset['annotations']\n",
    "        else:\n",
    "            if not len(imgIds) == 0:\n",
    "                anns = sum([self.imgToQA[imgId] for imgId in imgIds if imgId in self.imgToQA], [])\n",
    "            else:\n",
    "                anns = self.dataset['annotations']\n",
    "            anns = anns if len(quesTypes) == 0 else [ann for ann in anns if ann['question_type'] in quesTypes]\n",
    "            anns = anns if len(ansTypes) == 0 else [ann for ann in anns if ann['answer_type'] in ansTypes]\n",
    "        ids = [ann['question_id'] for ann in anns]\n",
    "        return ids\n",
    "\n",
    "    def getImgIds(self, quesIds=[], quesTypes=[], ansTypes=[]):\n",
    "        \"\"\"\n",
    "        Get image ids that satisfy given filter conditions.\n",
    "        \"\"\"\n",
    "        quesIds = quesIds if type(quesIds) == list else [quesIds]\n",
    "        quesTypes = quesTypes if type(quesTypes) == list else [quesTypes]\n",
    "        ansTypes = ansTypes if type(ansTypes) == list else [ansTypes]\n",
    "\n",
    "        if len(quesIds) == len(quesTypes) == len(ansTypes) == 0:\n",
    "            anns = self.dataset['annotations']\n",
    "        else:\n",
    "            if not len(quesIds) == 0:\n",
    "                anns = [self.qa[quesId] for quesId in quesIds if quesId in self.qa]\n",
    "            else:\n",
    "                anns = self.dataset['annotations']\n",
    "            anns = anns if len(quesTypes) == 0 else [ann for ann in anns if ann['question_type'] in quesTypes]\n",
    "            anns = anns if len(ansTypes) == 0 else [ann for ann in anns if ann['answer_type'] in ansTypes]\n",
    "        ids = [ann['image_id'] for ann in anns]\n",
    "        return ids\n",
    "\n",
    "    def loadQA(self, ids=[]):\n",
    "        \"\"\"\n",
    "        Load questions and answers with the specified question ids.\n",
    "        \"\"\"\n",
    "        if type(ids) == list:\n",
    "            return [self.qa[id] for id in ids]\n",
    "        elif type(ids) == int:\n",
    "            return [self.qa[ids]]\n",
    "\n",
    "    def showQA(self, anns):\n",
    "        \"\"\"\n",
    "        Display the specified annotations.\n",
    "        \"\"\"\n",
    "        if len(anns) == 0:\n",
    "            return 0\n",
    "        for ann in anns:\n",
    "            quesId = ann['question_id']\n",
    "            print(f\"Question: {self.qqa[quesId]['question']}\")\n",
    "            for ans in ann['answers']:\n",
    "                print(f\"Answer {ans['answer_id']}: {ans['answer']}\")\n",
    "\n",
    "    def loadRes(self, resFile, quesFile):\n",
    "        \"\"\"\n",
    "        Load result file and return a result object.\n",
    "        \"\"\"\n",
    "        res = VQA()\n",
    "        res.questions = json.load(open(quesFile))\n",
    "        res.dataset['info'] = copy.deepcopy(self.questions['info'])\n",
    "        res.dataset['task_type'] = copy.deepcopy(self.questions['task_type'])\n",
    "        res.dataset['data_type'] = copy.deepcopy(self.questions['data_type'])\n",
    "        res.dataset['data_subtype'] = copy.deepcopy(self.questions['data_subtype'])\n",
    "        res.dataset['license'] = copy.deepcopy(self.questions['license'])\n",
    "\n",
    "        print('Loading and preparing results...')\n",
    "        time_t = datetime.datetime.utcnow()\n",
    "        anns = json.load(open(resFile))\n",
    "        assert type(anns) == list, 'results is not an array of objects'\n",
    "        annsQuesIds = [ann['question_id'] for ann in anns]\n",
    "        \n",
    "        assert set(annsQuesIds) == set(self.getQuesIds()), \\\n",
    "            'Results do not correspond to current VQA set. Either the results do not have predictions for all question ids in annotation file or there is at least one question id that does not belong to the question ids in the annotation file.'\n",
    "        \n",
    "        for ann in anns:\n",
    "            quesId = ann['question_id']\n",
    "            if res.dataset.get('annotations') is None:\n",
    "                res.dataset['annotations'] = []\n",
    "            res.dataset['annotations'].append({\n",
    "                'question_id': quesId,\n",
    "                'answer': ann['answer']\n",
    "            })\n",
    "        \n",
    "        print('DONE (t={:0.2f}s)'.format((datetime.datetime.utcnow() - time_t).total_seconds()))\n",
    "        res.createIndex()\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download_data"
   },
   "source": [
    "## 5. Download VQAv2 Data\n",
    "\n",
    "Download the annotations and questions files from the VQA website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_vqa_data"
   },
   "outputs": [],
   "source": [
    "# Download VQAv2 validation annotations and questions\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('vqa_data', exist_ok=True)\n",
    "\n",
    "# Download validation annotations\n",
    "print('Downloading VQAv2 validation annotations...')\n",
    "ann_url = 'https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Annotations_Val_mscoco.zip'\n",
    "urllib.request.urlretrieve(ann_url, 'vqa_data/v2_Annotations_Val_mscoco.zip')\n",
    "\n",
    "# Download validation questions\n",
    "print('Downloading VQAv2 validation questions...')\n",
    "ques_url = 'https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Questions_Val_mscoco.zip'\n",
    "urllib.request.urlretrieve(ques_url, 'vqa_data/v2_Questions_Val_mscoco.zip')\n",
    "\n",
    "# Extract files\n",
    "print('Extracting files...')\n",
    "with zipfile.ZipFile('vqa_data/v2_Annotations_Val_mscoco.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('vqa_data/')\n",
    "\n",
    "with zipfile.ZipFile('vqa_data/v2_Questions_Val_mscoco.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('vqa_data/')\n",
    "\n",
    "print('Download complete!')\n",
    "print('\\nAnnotation file: vqa_data/v2_mscoco_val2014_annotations.json')\n",
    "print('Question file: vqa_data/v2_OpenEnded_mscoco_val2014_questions.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model_inference"
   },
   "source": [
    "## 6. Run Your Model Inference (Qwen-VL)\n",
    "\n",
    "Generate answers using your model. This section is adapted from your existing notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_model"
   },
   "outputs": [],
   "source": [
    "# Load your Qwen-VL model\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import torch\n",
    "\n",
    "# Load model\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Load processor\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "generate_predictions"
   },
   "outputs": [],
   "source": [
    "# Generate predictions on VQAv2 validation set\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "\n",
    "# Load VQA dataset\n",
    "vqa_dataset = load_dataset(\"HuggingFaceM4/VQAv2\", split=\"validation\")\n",
    "\n",
    "# Generate predictions\n",
    "results = []\n",
    "num_samples = 1000  # Adjust this number for full evaluation\n",
    "\n",
    "print(f\"Generating predictions for {num_samples} samples...\")\n",
    "for i in tqdm(range(num_samples)):\n",
    "    sample = vqa_dataset[i]\n",
    "    \n",
    "    # Prepare messages\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": sample['image']},\n",
    "                {\"type\": \"text\", \"text\": sample['question']},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Prepare for inference\n",
    "    text = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    inputs = inputs.to(\"cuda\")\n",
    "\n",
    "    # Generate\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "    )[0]\n",
    "    \n",
    "    # Store result\n",
    "    results.append({\n",
    "        'question_id': sample['question_id'],\n",
    "        'answer': output_text.strip()\n",
    "    })\n",
    "\n",
    "# Save results to file\n",
    "with open('vqa_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"\\nGenerated {len(results)} predictions\")\n",
    "print(\"Results saved to: vqa_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluation"
   },
   "source": [
    "## 7. Evaluate Results\n",
    "\n",
    "Use the VQA evaluation code to compute accuracy on your verbose answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_evaluation"
   },
   "outputs": [],
   "source": [
    "# Run VQA evaluation\n",
    "annFile = 'vqa_data/v2_mscoco_val2014_annotations.json'\n",
    "quesFile = 'vqa_data/v2_OpenEnded_mscoco_val2014_questions.json'\n",
    "resFile = 'vqa_results.json'\n",
    "\n",
    "# Create VQA object for ground truth\n",
    "print(\"Loading ground truth annotations...\")\n",
    "vqa = VQA(annFile, quesFile)\n",
    "\n",
    "# Load results\n",
    "print(\"\\nLoading results...\")\n",
    "vqaRes = vqa.loadRes(resFile, quesFile)\n",
    "\n",
    "# Create VQAEval object\n",
    "print(\"\\nEvaluating results...\")\n",
    "vqaEval = VQAEval(vqa, vqaRes, n=2)\n",
    "\n",
    "# Evaluate\n",
    "vqaEval.evaluate()\n",
    "\n",
    "# Print results\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nOverall Accuracy: {vqaEval.accuracy['overall']:.2f}%\")\n",
    "\n",
    "print(\"\\nAccuracy by Question Type:\")\n",
    "for quesType in sorted(vqaEval.accuracy['perQuestionType']):\n",
    "    print(f\"  {quesType}: {vqaEval.accuracy['perQuestionType'][quesType]:.2f}%\")\n",
    "\n",
    "print(\"\\nAccuracy by Answer Type:\")\n",
    "for ansType in sorted(vqaEval.accuracy['perAnswerType']):\n",
    "    print(f\"  {ansType}: {vqaEval.accuracy['perAnswerType'][ansType]:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save_results"
   },
   "source": [
    "## 8. Save Detailed Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_detailed_results"
   },
   "outputs": [],
   "source": [
    "# Save detailed evaluation results\n",
    "detailed_results = {\n",
    "    'overall_accuracy': vqaEval.accuracy['overall'],\n",
    "    'per_question_type': vqaEval.accuracy['perQuestionType'],\n",
    "    'per_answer_type': vqaEval.accuracy['perAnswerType'],\n",
    "    'per_question_accuracy': vqaEval.evalQA\n",
    "}\n",
    "\n",
    "with open('vqa_evaluation_results.json', 'w') as f:\n",
    "    json.dump(detailed_results, f, indent=2)\n",
    "\n",
    "print(\"Detailed results saved to: vqa_evaluation_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "visualize"
   },
   "source": [
    "## 9. Visualize Results (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot_results"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Plot accuracy by question type\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Question type accuracy\n",
    "quesTypes = list(vqaEval.accuracy['perQuestionType'].keys())\n",
    "quesAccs = list(vqaEval.accuracy['perQuestionType'].values())\n",
    "\n",
    "ax1.barh(quesTypes, quesAccs, color='steelblue')\n",
    "ax1.set_xlabel('Accuracy (%)')\n",
    "ax1.set_title('Accuracy by Question Type')\n",
    "ax1.set_xlim([0, 100])\n",
    "\n",
    "# Answer type accuracy\n",
    "ansTypes = list(vqaEval.accuracy['perAnswerType'].keys())\n",
    "ansAccs = list(vqaEval.accuracy['perAnswerType'].values())\n",
    "\n",
    "ax2.bar(ansTypes, ansAccs, color='coral')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.set_title('Accuracy by Answer Type')\n",
    "ax2.set_ylim([0, 100])\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('vqa_accuracy_plots.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Plots saved to: vqa_accuracy_plots.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "example_predictions"
   },
   "source": [
    "## 10. View Example Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "show_examples"
   },
   "outputs": [],
   "source": [
    "# Show some example predictions\n",
    "import random\n",
    "from IPython.display import display\n",
    "\n",
    "num_examples = 5\n",
    "example_ids = random.sample(list(vqaEval.evalQA.keys()), num_examples)\n",
    "\n",
    "print(\"Example Predictions:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for quesId in example_ids:\n",
    "    # Get question\n",
    "    question = vqa.qqa[quesId]['question']\n",
    "    \n",
    "    # Get ground truth answers\n",
    "    gt_answers = [ans['answer'] for ans in vqa.qa[quesId]['answers']]\n",
    "    \n",
    "    # Get predicted answer\n",
    "    pred_answer = vqaRes.qa[quesId]['answer']\n",
    "    \n",
    "    # Get accuracy\n",
    "    accuracy = vqaEval.evalQA[quesId]\n",
    "    \n",
    "    print(f\"\\nQuestion ID: {quesId}\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Ground Truth Answers: {', '.join(set(gt_answers))}\")\n",
    "    print(f\"Predicted Answer: {pred_answer}\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "    print(\"-\"*80)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
